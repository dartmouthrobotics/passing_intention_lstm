{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "756d8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential packages\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab6f09",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb0cf124",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"aisdk_20180101\"\n",
    "howmany=100\n",
    "using_file_indices = np.arange(0,howmany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7b4d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire_pass = pd.DataFrame()\n",
    "\n",
    "# https://www.w3resource.com/pandas/dataframe/dataframe-to_pickle.php\n",
    "for using_file_idx in using_file_indices:\n",
    "    unpickled_df = pd.read_pickle(\"./dk_csv_20180101/aisdk_20180101_{}.pkl\".format(str(using_file_idx)))\n",
    "    unpickled_df\n",
    "    df_entire_pass = pd.concat([df_entire_pass, unpickled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37ad7a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:30</th>\n",
       "      <td>4.214136</td>\n",
       "      <td>2.391927</td>\n",
       "      <td>-0.009386</td>\n",
       "      <td>-0.006086</td>\n",
       "      <td>4.845643</td>\n",
       "      <td>0.516252</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>145.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:45</th>\n",
       "      <td>4.086469</td>\n",
       "      <td>2.312937</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>4.695626</td>\n",
       "      <td>0.515043</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>145.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:00</th>\n",
       "      <td>3.972708</td>\n",
       "      <td>2.235017</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>4.558257</td>\n",
       "      <td>0.512460</td>\n",
       "      <td>-0.002584</td>\n",
       "      <td>147.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:15</th>\n",
       "      <td>3.857949</td>\n",
       "      <td>2.154808</td>\n",
       "      <td>-0.007651</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>4.418933</td>\n",
       "      <td>0.509374</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>150.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:30</th>\n",
       "      <td>3.730337</td>\n",
       "      <td>2.078144</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>4.270140</td>\n",
       "      <td>0.508272</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>150.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:03:15</th>\n",
       "      <td>-0.262677</td>\n",
       "      <td>0.819739</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.860797</td>\n",
       "      <td>1.880898</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:03:30</th>\n",
       "      <td>-0.282814</td>\n",
       "      <td>0.865512</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.910546</td>\n",
       "      <td>1.886618</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:03:45</th>\n",
       "      <td>-0.297036</td>\n",
       "      <td>0.894617</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.942640</td>\n",
       "      <td>1.891370</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:04:00</th>\n",
       "      <td>-0.310155</td>\n",
       "      <td>0.921015</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.971835</td>\n",
       "      <td>1.895622</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:04:15</th>\n",
       "      <td>-0.310155</td>\n",
       "      <td>0.921015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971835</td>\n",
       "      <td>1.895622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17276 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            x         y       v_x       v_y  rel_dist  \\\n",
       "Timestamp                                                               \n",
       "2018-01-01 23:34:30  4.214136  2.391927 -0.009386 -0.006086  4.845643   \n",
       "2018-01-01 23:34:45  4.086469  2.312937 -0.008511 -0.005266  4.695626   \n",
       "2018-01-01 23:35:00  3.972708  2.235017 -0.007584 -0.005195  4.558257   \n",
       "2018-01-01 23:35:15  3.857949  2.154808 -0.007651 -0.005347  4.418933   \n",
       "2018-01-01 23:35:30  3.730337  2.078144 -0.008507 -0.005111  4.270140   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2018-01-01 19:03:15 -0.262677  0.819739 -0.001342  0.003051  0.860797   \n",
       "2018-01-01 19:03:30 -0.282814  0.865512 -0.001342  0.003051  0.910546   \n",
       "2018-01-01 19:03:45 -0.297036  0.894617 -0.000948  0.001940  0.942640   \n",
       "2018-01-01 19:04:00 -0.310155  0.921015 -0.000875  0.001760  0.971835   \n",
       "2018-01-01 19:04:15 -0.310155  0.921015  0.000000  0.000000  0.971835   \n",
       "\n",
       "                     rel_bearing  rel_bearing_diff  Heading  valid  obj_index  \\\n",
       "Timestamp                                                                       \n",
       "2018-01-01 23:34:30     0.516252         -0.001975    145.0   True          0   \n",
       "2018-01-01 23:34:45     0.515043         -0.001209    145.5   True          0   \n",
       "2018-01-01 23:35:00     0.512460         -0.002584    147.0   True          0   \n",
       "2018-01-01 23:35:15     0.509374         -0.003086    150.5   True          0   \n",
       "2018-01-01 23:35:30     0.508272         -0.001102    150.0   True          0   \n",
       "...                          ...               ...      ...    ...        ...   \n",
       "2018-01-01 19:03:15     1.880898          0.006422     72.0   True         99   \n",
       "2018-01-01 19:03:30     1.886618          0.005720     72.0   True         99   \n",
       "2018-01-01 19:03:45     1.891370          0.004752     72.0   True         99   \n",
       "2018-01-01 19:04:00     1.895622          0.004252     72.0   True         99   \n",
       "2018-01-01 19:04:15     1.895622          0.000000     72.0   True         99   \n",
       "\n",
       "                    label  \n",
       "Timestamp                  \n",
       "2018-01-01 23:34:30     L  \n",
       "2018-01-01 23:34:45     L  \n",
       "2018-01-01 23:35:00     L  \n",
       "2018-01-01 23:35:15     L  \n",
       "2018-01-01 23:35:30     L  \n",
       "...                   ...  \n",
       "2018-01-01 19:03:15     L  \n",
       "2018-01-01 19:03:30     L  \n",
       "2018-01-01 19:03:45     L  \n",
       "2018-01-01 19:04:00     L  \n",
       "2018-01-01 19:04:15     L  \n",
       "\n",
       "[17276 rows x 11 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entire_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde3dd4",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a58175c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_NED_to_Robotic(angle):\n",
    "\n",
    "    \"\"\"\n",
    "    convert radian angle based on NED frame to radian angle based on the robotics frame\n",
    "    \"\"\"\n",
    "\n",
    "    if 0 <= angle <= np.pi * 1/2:\n",
    "        converted_angle = np.pi * 1/2 - angle\n",
    "        return converted_angle\n",
    "\n",
    "    elif np.pi * 1/2 < angle < np.pi * 3/2:\n",
    "        converted_angle = - angle + np.pi * 1/2\n",
    "        return converted_angle\n",
    "\n",
    "    else: # 270 <= angle < 360\n",
    "        converted_angle = (np.pi * 5/2) - angle\n",
    "        return converted_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04e6d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire_pass['heading_converted']  = np.deg2rad(df_entire_pass['Heading'])\n",
    "# lambda function method: very fast! \n",
    "# https://stackoverflow.com/questions/71249186/applying-function-to-column-in-a-dataframe\n",
    "df_entire_pass['heading_converted'] = df_entire_pass['heading_converted'].apply(convert_from_NED_to_Robotic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8034fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "      <th>heading_converted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:30</th>\n",
       "      <td>4.214136</td>\n",
       "      <td>2.391927</td>\n",
       "      <td>-0.009386</td>\n",
       "      <td>-0.006086</td>\n",
       "      <td>4.845643</td>\n",
       "      <td>0.516252</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>145.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.959931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:45</th>\n",
       "      <td>4.086469</td>\n",
       "      <td>2.312937</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>4.695626</td>\n",
       "      <td>0.515043</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>145.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.968658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:00</th>\n",
       "      <td>3.972708</td>\n",
       "      <td>2.235017</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>4.558257</td>\n",
       "      <td>0.512460</td>\n",
       "      <td>-0.002584</td>\n",
       "      <td>147.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.994838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:15</th>\n",
       "      <td>3.857949</td>\n",
       "      <td>2.154808</td>\n",
       "      <td>-0.007651</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>4.418933</td>\n",
       "      <td>0.509374</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>150.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.055924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:30</th>\n",
       "      <td>3.730337</td>\n",
       "      <td>2.078144</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>4.270140</td>\n",
       "      <td>0.508272</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>150.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.047198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:03:15</th>\n",
       "      <td>-0.262677</td>\n",
       "      <td>0.819739</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.860797</td>\n",
       "      <td>1.880898</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "      <td>0.314159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:03:30</th>\n",
       "      <td>-0.282814</td>\n",
       "      <td>0.865512</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.910546</td>\n",
       "      <td>1.886618</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "      <td>0.314159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:03:45</th>\n",
       "      <td>-0.297036</td>\n",
       "      <td>0.894617</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.942640</td>\n",
       "      <td>1.891370</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "      <td>0.314159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:04:00</th>\n",
       "      <td>-0.310155</td>\n",
       "      <td>0.921015</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.971835</td>\n",
       "      <td>1.895622</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "      <td>0.314159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 19:04:15</th>\n",
       "      <td>-0.310155</td>\n",
       "      <td>0.921015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971835</td>\n",
       "      <td>1.895622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>L</td>\n",
       "      <td>0.314159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17276 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            x         y       v_x       v_y  rel_dist  \\\n",
       "Timestamp                                                               \n",
       "2018-01-01 23:34:30  4.214136  2.391927 -0.009386 -0.006086  4.845643   \n",
       "2018-01-01 23:34:45  4.086469  2.312937 -0.008511 -0.005266  4.695626   \n",
       "2018-01-01 23:35:00  3.972708  2.235017 -0.007584 -0.005195  4.558257   \n",
       "2018-01-01 23:35:15  3.857949  2.154808 -0.007651 -0.005347  4.418933   \n",
       "2018-01-01 23:35:30  3.730337  2.078144 -0.008507 -0.005111  4.270140   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2018-01-01 19:03:15 -0.262677  0.819739 -0.001342  0.003051  0.860797   \n",
       "2018-01-01 19:03:30 -0.282814  0.865512 -0.001342  0.003051  0.910546   \n",
       "2018-01-01 19:03:45 -0.297036  0.894617 -0.000948  0.001940  0.942640   \n",
       "2018-01-01 19:04:00 -0.310155  0.921015 -0.000875  0.001760  0.971835   \n",
       "2018-01-01 19:04:15 -0.310155  0.921015  0.000000  0.000000  0.971835   \n",
       "\n",
       "                     rel_bearing  rel_bearing_diff  Heading  valid  obj_index  \\\n",
       "Timestamp                                                                       \n",
       "2018-01-01 23:34:30     0.516252         -0.001975    145.0   True          0   \n",
       "2018-01-01 23:34:45     0.515043         -0.001209    145.5   True          0   \n",
       "2018-01-01 23:35:00     0.512460         -0.002584    147.0   True          0   \n",
       "2018-01-01 23:35:15     0.509374         -0.003086    150.5   True          0   \n",
       "2018-01-01 23:35:30     0.508272         -0.001102    150.0   True          0   \n",
       "...                          ...               ...      ...    ...        ...   \n",
       "2018-01-01 19:03:15     1.880898          0.006422     72.0   True         99   \n",
       "2018-01-01 19:03:30     1.886618          0.005720     72.0   True         99   \n",
       "2018-01-01 19:03:45     1.891370          0.004752     72.0   True         99   \n",
       "2018-01-01 19:04:00     1.895622          0.004252     72.0   True         99   \n",
       "2018-01-01 19:04:15     1.895622          0.000000     72.0   True         99   \n",
       "\n",
       "                    label  heading_converted  \n",
       "Timestamp                                     \n",
       "2018-01-01 23:34:30     L          -0.959931  \n",
       "2018-01-01 23:34:45     L          -0.968658  \n",
       "2018-01-01 23:35:00     L          -0.994838  \n",
       "2018-01-01 23:35:15     L          -1.055924  \n",
       "2018-01-01 23:35:30     L          -1.047198  \n",
       "...                   ...                ...  \n",
       "2018-01-01 19:03:15     L           0.314159  \n",
       "2018-01-01 19:03:30     L           0.314159  \n",
       "2018-01-01 19:03:45     L           0.314159  \n",
       "2018-01-01 19:04:00     L           0.314159  \n",
       "2018-01-01 19:04:15     L           0.314159  \n",
       "\n",
       "[17276 rows x 12 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entire_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d392c52",
   "metadata": {},
   "source": [
    "## Train, test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "526595ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 80 \n",
      " test size: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([26, 86,  2, 55, 75, 93, 16, 73, 54, 95, 53, 92, 78, 13,  7, 30, 22,\n",
       "       24, 33,  8, 43, 62,  3, 71, 45, 48,  6, 99, 82, 76, 60, 80, 90, 68,\n",
       "       51, 27, 18, 56, 63, 74,  1, 61, 42, 41,  4, 15, 17, 40, 38,  5, 91,\n",
       "       59,  0, 34, 28, 50, 11, 35, 23, 52, 10, 31, 66, 57, 79, 85, 32, 84,\n",
       "       14, 89, 19, 29, 49, 97, 98, 69, 20, 94, 72, 77])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/how-to-randomly-select-elements-of-an-array-with-numpy-in-python/\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# for train, test data\n",
    "use_df = df_entire_pass.loc[(df_entire_pass.valid == True)] # to be used, valid df\n",
    "unique_id = use_df.obj_index.unique()\n",
    "\n",
    "# data split\n",
    "train_data_size = int(len(unique_id) * 0.8)\n",
    "test_data_size = len(unique_id) - train_data_size\n",
    "print(\"train size: {} \\n test size: {}\".format(train_data_size, test_data_size))\n",
    "\n",
    "# split obj indexes\n",
    "train_obj_id = np.random.choice(unique_id, size = train_data_size, replace=False)\n",
    "test_obj_id = np.setdiff1d(unique_id, train_obj_id)\n",
    "train_obj_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57ab2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory_before_pass(df_input, obj_id_array):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - df_input: data_frame for training or test\n",
    "\n",
    "    Returns:\n",
    "        - cropped dataframe with position only ahead for predicting the direction\n",
    "    \"\"\"\n",
    "    df_cropped = pd.DataFrame()\n",
    "    for idx, obj_id in enumerate(obj_id_array):\n",
    "        # TODO backside too\n",
    "        df_cropped = pd.concat([df_cropped, df_input.loc[(df_input['obj_index'] == obj_id) & \\\n",
    "                                                         (df_input['x'] > 0.0)]], ignore_index=True)\n",
    "    return df_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "adddad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dimension(x_data, y_data):\n",
    "    if len(x_data) == len(y_data):\n",
    "        print(\"Input, output dimension is same. good to go\")\n",
    "    else:\n",
    "        raise ValueError(\"Input, output dimension is different. Double check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27158cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "      <th>heading_converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.698369</td>\n",
       "      <td>-1.403258</td>\n",
       "      <td>-0.009192</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>4.903448</td>\n",
       "      <td>-0.290235</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>210.0</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.565623</td>\n",
       "      <td>-1.343425</td>\n",
       "      <td>-0.008850</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>4.759171</td>\n",
       "      <td>-0.286171</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>210.0</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.422403</td>\n",
       "      <td>-1.278315</td>\n",
       "      <td>-0.009548</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>4.603449</td>\n",
       "      <td>-0.281385</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>210.0</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.279420</td>\n",
       "      <td>-1.213489</td>\n",
       "      <td>-0.009532</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>4.448145</td>\n",
       "      <td>-0.276310</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>210.0</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.138647</td>\n",
       "      <td>-1.151726</td>\n",
       "      <td>-0.009385</td>\n",
       "      <td>0.004118</td>\n",
       "      <td>4.295914</td>\n",
       "      <td>-0.271418</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>210.0</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10445</th>\n",
       "      <td>0.336464</td>\n",
       "      <td>0.919474</td>\n",
       "      <td>-0.004602</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.979102</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.068773</td>\n",
       "      <td>193.0</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.797689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10446</th>\n",
       "      <td>0.262734</td>\n",
       "      <td>0.929513</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.965931</td>\n",
       "      <td>1.295325</td>\n",
       "      <td>0.075325</td>\n",
       "      <td>193.0</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.797689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10447</th>\n",
       "      <td>0.198039</td>\n",
       "      <td>0.939042</td>\n",
       "      <td>-0.004313</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.959697</td>\n",
       "      <td>1.362947</td>\n",
       "      <td>0.067623</td>\n",
       "      <td>193.5</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.806416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10448</th>\n",
       "      <td>0.129021</td>\n",
       "      <td>0.949109</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.957838</td>\n",
       "      <td>1.435685</td>\n",
       "      <td>0.072738</td>\n",
       "      <td>194.0</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.815142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10449</th>\n",
       "      <td>0.050902</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.963881</td>\n",
       "      <td>1.517962</td>\n",
       "      <td>0.082278</td>\n",
       "      <td>193.5</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.806416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10450 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x         y       v_x       v_y  rel_dist  rel_bearing  \\\n",
       "0      4.698369 -1.403258 -0.009192  0.004104  4.903448    -0.290235   \n",
       "1      4.565623 -1.343425 -0.008850  0.003989  4.759171    -0.286171   \n",
       "2      4.422403 -1.278315 -0.009548  0.004341  4.603449    -0.281385   \n",
       "3      4.279420 -1.213489 -0.009532  0.004322  4.448145    -0.276310   \n",
       "4      4.138647 -1.151726 -0.009385  0.004118  4.295914    -0.271418   \n",
       "...         ...       ...       ...       ...       ...          ...   \n",
       "10445  0.336464  0.919474 -0.004602  0.000695  0.979102     1.220000   \n",
       "10446  0.262734  0.929513 -0.004915  0.000669  0.965931     1.295325   \n",
       "10447  0.198039  0.939042 -0.004313  0.000635  0.959697     1.362947   \n",
       "10448  0.129021  0.949109 -0.004601  0.000671  0.957838     1.435685   \n",
       "10449  0.050902  0.962536 -0.005208  0.000895  0.963881     1.517962   \n",
       "\n",
       "       rel_bearing_diff  Heading  valid  obj_index label  heading_converted  \n",
       "0              0.003865    210.0   True         26     L          -2.094395  \n",
       "1              0.004064    210.0   True         26     L          -2.094395  \n",
       "2              0.004786    210.0   True         26     L          -2.094395  \n",
       "3              0.005074    210.0   True         26     L          -2.094395  \n",
       "4              0.004892    210.0   True         26     L          -2.094395  \n",
       "...                 ...      ...    ...        ...   ...                ...  \n",
       "10445          0.068773    193.0   True         77     L          -1.797689  \n",
       "10446          0.075325    193.0   True         77     L          -1.797689  \n",
       "10447          0.067623    193.5   True         77     L          -1.806416  \n",
       "10448          0.072738    194.0   True         77     L          -1.815142  \n",
       "10449          0.082278    193.5   True         77     L          -1.806416  \n",
       "\n",
       "[10450 rows x 12 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "df_cropped_train = get_trajectory_before_pass(df_entire_pass, train_obj_id)\n",
    "df_cropped_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "28e857f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(df_cropped_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f11c1f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(df_cropped_train['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "336d5754",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "      <th>heading_converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.531333</td>\n",
       "      <td>4.889139</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.007141</td>\n",
       "      <td>4.917926</td>\n",
       "      <td>1.462545</td>\n",
       "      <td>-0.002815</td>\n",
       "      <td>90.625000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.010908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.531943</td>\n",
       "      <td>4.777563</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.007438</td>\n",
       "      <td>4.807085</td>\n",
       "      <td>1.459911</td>\n",
       "      <td>-0.002634</td>\n",
       "      <td>90.937500</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.531469</td>\n",
       "      <td>4.642245</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.009021</td>\n",
       "      <td>4.672568</td>\n",
       "      <td>1.456807</td>\n",
       "      <td>-0.003104</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.021817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.534526</td>\n",
       "      <td>4.534370</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.007192</td>\n",
       "      <td>4.565767</td>\n",
       "      <td>1.453455</td>\n",
       "      <td>-0.003353</td>\n",
       "      <td>91.562500</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.027271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.537584</td>\n",
       "      <td>4.426495</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.007192</td>\n",
       "      <td>4.459020</td>\n",
       "      <td>1.449941</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>91.875000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.032725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>0.094810</td>\n",
       "      <td>-0.029229</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.099214</td>\n",
       "      <td>-0.299046</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>97.163636</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.125029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>0.096544</td>\n",
       "      <td>-0.029880</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.101062</td>\n",
       "      <td>-0.300143</td>\n",
       "      <td>-0.001096</td>\n",
       "      <td>95.109091</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.089170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>0.098277</td>\n",
       "      <td>-0.030530</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.102910</td>\n",
       "      <td>-0.301199</td>\n",
       "      <td>-0.001057</td>\n",
       "      <td>93.054545</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.053312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>0.100011</td>\n",
       "      <td>-0.031180</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.104758</td>\n",
       "      <td>-0.302219</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.017453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>0.042027</td>\n",
       "      <td>-0.023922</td>\n",
       "      <td>-0.003866</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.048358</td>\n",
       "      <td>-0.517463</td>\n",
       "      <td>-0.215244</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.488692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5094 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x         y       v_x       v_y  rel_dist  rel_bearing  \\\n",
       "0     0.531333  4.889139  0.000172 -0.007141  4.917926     1.462545   \n",
       "1     0.531943  4.777563  0.000041 -0.007438  4.807085     1.459911   \n",
       "2     0.531469  4.642245 -0.000032 -0.009021  4.672568     1.456807   \n",
       "3     0.534526  4.534370  0.000204 -0.007192  4.565767     1.453455   \n",
       "4     0.537584  4.426495  0.000204 -0.007192  4.459020     1.449941   \n",
       "...        ...       ...       ...       ...       ...          ...   \n",
       "5089  0.094810 -0.029229  0.000007  0.000066  0.099214    -0.299046   \n",
       "5090  0.096544 -0.029880  0.000116 -0.000043  0.101062    -0.300143   \n",
       "5091  0.098277 -0.030530  0.000116 -0.000043  0.102910    -0.301199   \n",
       "5092  0.100011 -0.031180  0.000116 -0.000043  0.104758    -0.302219   \n",
       "5093  0.042027 -0.023922 -0.003866  0.000484  0.048358    -0.517463   \n",
       "\n",
       "      rel_bearing_diff     Heading  valid  obj_index label  heading_converted  \n",
       "0            -0.002815   90.625000   True          9     R          -0.010908  \n",
       "1            -0.002634   90.937500   True          9     R          -0.016362  \n",
       "2            -0.003104   91.250000   True          9     R          -0.021817  \n",
       "3            -0.003353   91.562500   True          9     R          -0.027271  \n",
       "4            -0.003513   91.875000   True          9     R          -0.032725  \n",
       "...                ...         ...    ...        ...   ...                ...  \n",
       "5089          0.009800   97.163636   True         96     L          -0.125029  \n",
       "5090         -0.001096   95.109091   True         96     L          -0.089170  \n",
       "5091         -0.001057   93.054545   True         96     L          -0.053312  \n",
       "5092         -0.001019   91.000000   True         96     L          -0.017453  \n",
       "5093         -0.215244  118.000000   True         96     L          -0.488692  \n",
       "\n",
       "[5094 rows x 12 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "df_cropped_test = get_trajectory_before_pass(df_entire_pass, test_obj_id)\n",
    "df_cropped_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9785d11",
   "metadata": {},
   "source": [
    "## Train, test data onehot encoding and extract specific cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff3e75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input, output dimension is same. good to go\n",
      "Input, output dimension is same. good to go\n"
     ]
    }
   ],
   "source": [
    "# columns to extract\n",
    "columns_for_x = ['x', 'y', 'rel_dist', 'rel_bearing', 'heading_converted']\n",
    "# columns_for_x = ['r_value', 'atan']\n",
    "columns_for_y = ['label']\n",
    "\n",
    "# one hot encoding for y target\n",
    "one_hot_lookup = torch.eye(2).tolist()\n",
    "\n",
    "# df to array\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "#### train dataset\n",
    "for obj_id, df_by_ID in df_cropped_train.groupby('obj_index'):\n",
    "    X_train.append(df_by_ID[columns_for_x].values.tolist())\n",
    "    # y_train.append(16one_hot_lookup[0] if df_by_ID[columns_for_y].values[-1][0] == \"L\" else one_hot_lookup[1]) # last row, only char\n",
    "    y_train.append(0 if df_by_ID[columns_for_y].values[-1][0] == \"L\" else 1) # last row, only char\n",
    "\n",
    "    #### double check train dataset\n",
    "check_dimension(X_train, y_train)\n",
    "\n",
    "#### test dataset\n",
    "for obj_id, df_by_ID in df_cropped_test.groupby('obj_index'):\n",
    "    X_test.append(df_by_ID[columns_for_x].values.tolist())\n",
    "    # y_test.append(one_hot_lookup[0] if df_by_ID[columns_for_y].values[-1][0] == \"L\" else one_hot_lookup[1]) # last row, only char\n",
    "    y_test.append(0 if df_by_ID[columns_for_y].values[-1][0] == \"L\" else 1) # last row, only char\n",
    "\n",
    "#### double check test dataset\n",
    "check_dimension(X_test, y_test)\n",
    "\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb68535",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032b8cd",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b59c5a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence_length = 10 # number of timestamps # padding\n",
    "input_size = 5 # number of columns, features\n",
    "batch_size = 32 # number of samples sent to the model at one time 32\n",
    "\n",
    "hidden_size = 64 # dimension of hidden state # 500\n",
    "num_layers = 10 # total layer\n",
    "num_classes = 1 # output class (L or R)\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "clip=1 # gradient clipping\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38661355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "# sequences: List of variable-length sequences\n",
    "# targets: List of target labels\n",
    "\n",
    "def pad_sequential_data(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_data: (list) training or test X data, sequential, not equal length\n",
    "        y_data: (list) training or test y data\n",
    "\n",
    "    Returns:\n",
    "        - same length sequence for X_data\n",
    "\n",
    "    reference: https://chat.openai.com/c/235f65e4-3a26-4418-a88c-ecf521cc5d8d\n",
    "    \"\"\"\n",
    "    sequences = [torch.tensor(seq) for seq in X_data]\n",
    "    targets = torch.tensor(y_data)\n",
    "\n",
    "    #### Sort sequences by length in descending order\n",
    "    # sequences[i] indicatess i th object (variable length)\n",
    "    # sequence[i][0] (dimension: feature numbers)\n",
    "\n",
    "    # sorted_indices = sorted(range(len(sequences)), key=lambda i: len(sequences[i][0]), reverse=True)\n",
    "    sorted_indices = sorted(range(len(sequences)), key=lambda i: len(sequences[i]), reverse=True)\n",
    "\n",
    "    #### sorted as per length\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = targets[sorted_indices]\n",
    "\n",
    "    #### Pad the sequences to make them the same length (zero padding as default)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b37d305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3253, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Create a TensorDataset from the padded sequences and targets\n",
    "padded_sequences, targets = pad_sequential_data(X_train, y_train)\n",
    "train_dataset = TensorDataset(padded_sequences, targets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "padded_sequences_test, targets_test = pad_sequential_data(X_test, y_test)\n",
    "test_dataset = TensorDataset(padded_sequences_test, targets_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "padded_sequences_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f861287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312683a5",
   "metadata": {},
   "source": [
    "### RNN define and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "950e1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an RNN model\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         # https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402/2\n",
    "#         # self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "#         self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         ##### RNN\n",
    "#         # _, hidden = self.rnn(x)\n",
    "#         # output = self.fc(hidden.squeeze(0))\n",
    "#         # return output\n",
    "\n",
    "\n",
    "#         ##### LSTM\n",
    "#         # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "#         # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "#         h0 = torch.zeros(self.num_layers, x.data.size(0), self.hidden_size).to(device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.data.size(0), self.hidden_size).to(device)\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, hidden = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "#         # Decode the hidden state of the last time step\n",
    "#         # out = self.batch_norm(out[:, -1, :])\n",
    "#         # out = self.fc(out)\n",
    "#         # // up to here batchnorm use\n",
    "\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         out = self.sigmoid(out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48fb73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "# print(model)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# # criterion = nn.BCEWithLogitsLoss() ## using\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# import torch.optim as optim\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c6ecc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train mode\n",
    "# model.train()\n",
    "\n",
    "# loss_history =torch.tensor([])\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch_sequences, batch_targets in train_loader:\n",
    "#         # print(batch_sequences.size())\n",
    "#         # print(batch_targets.size())\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # 1) Pack the sequences before passing them to the RNN\n",
    "#         # TODO do we really need this?\n",
    "#         sequence_lengths = [len(seq) for seq in batch_sequences]\n",
    "#         packed_sequences = pack_padded_sequence(batch_sequences, sequence_lengths, batch_first=True, enforce_sorted=False).to(device)\n",
    "\n",
    "#         batch_sequences = batch_sequences.to(device)\n",
    "#         batch_targets = batch_targets.to(device)\n",
    "\n",
    "#         # 2) Forward pass\n",
    "#         output = model(batch_sequences)\n",
    "#         # output = model(packed_sequences)\n",
    "#         # Compute the loss\n",
    "#         loss = criterion(output.squeeze(), batch_targets.float())\n",
    "\n",
    "#         # 3) Backward pass and optimization\n",
    "#         loss.backward()\n",
    "\n",
    "          # 4) Gradient clipping\n",
    "#         # nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#     loss_history = torch.cat([loss_history, torch.tensor([loss.item()]).float()], dim=0)\n",
    "\n",
    "#     # Print the loss for every epoch\n",
    "#     if epoch % 1 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# #     scheduler.step()\n",
    "# #     print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "# #     if epoch % 5 == 0:print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0b8e8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sequence_length = 10 # number of timestamps # padding\n",
    "# input_size = 5 # number of columns, features\n",
    "# batch_size = 32 # number of samples sent to the model at one time\n",
    "\n",
    "# hidden_size = 500 # dimension of hidden state\n",
    "# num_layers = 10 # total layer\n",
    "# num_classes = 1 # output class (L or R)\n",
    "# num_epochs = 300\n",
    "# learning_rate = 0.008\n",
    "# clip=1 # gradient clipping\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaf662",
   "metadata": {},
   "source": [
    "### LSTM 2D model with packed padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0b918f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM2D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM2D, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_sequences = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        packed_out, _ = self.lstm(packed_sequences, (h0, c0))\n",
    "        \n",
    "#         # Decode the hidden state of the last time step\n",
    "#         # out = self.batch_norm(out[:, -1, :])\n",
    "#         # out = self.fc(out)\n",
    "#         # // up to here batchnorm use\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "#         out = self.batch_norm(out)\n",
    "        out = self.fc(out)\n",
    "        # https://stackoverflow.com/questions/66456541/runtimeerror-cuda-error-device-side-assert-triggered-on-loss-function\n",
    "        out = self.sigmoid(out) # ouput [0, 1]\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5f8f5083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM2D(\n",
      "  (lstm): LSTM(5, 64, num_layers=10, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM2D(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4302bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.7043993473052979\n",
      "Epoch 2/1000, Loss: 0.7039151191711426\n",
      "Epoch 3/1000, Loss: 0.7013883590698242\n",
      "Epoch 4/1000, Loss: 0.700514554977417\n",
      "Epoch 5/1000, Loss: 0.6987202167510986\n",
      "Epoch 6/1000, Loss: 0.6958824992179871\n",
      "Epoch 7/1000, Loss: 0.6951488256454468\n",
      "Epoch 8/1000, Loss: 0.6938349008560181\n",
      "Epoch 9/1000, Loss: 0.6919913291931152\n",
      "Epoch 10/1000, Loss: 0.6892048120498657\n",
      "Epoch 11/1000, Loss: 0.7044572830200195\n",
      "Epoch 12/1000, Loss: 0.6867601275444031\n",
      "Epoch 13/1000, Loss: 0.6845365762710571\n",
      "Epoch 14/1000, Loss: 0.6808549165725708\n",
      "Epoch 15/1000, Loss: 0.6783273220062256\n",
      "Epoch 16/1000, Loss: 0.676868736743927\n",
      "Epoch 17/1000, Loss: 0.6749128699302673\n",
      "Epoch 18/1000, Loss: 0.7278903126716614\n",
      "Epoch 19/1000, Loss: 0.6725388765335083\n",
      "Epoch 20/1000, Loss: 0.7214378118515015\n",
      "Epoch 21/1000, Loss: 0.7174361944198608\n",
      "Epoch 22/1000, Loss: 0.6725666522979736\n",
      "Epoch 23/1000, Loss: 0.6734480857849121\n",
      "Epoch 24/1000, Loss: 0.6710433959960938\n",
      "Epoch 25/1000, Loss: 0.6707892417907715\n",
      "Epoch 26/1000, Loss: 0.6691935062408447\n",
      "Epoch 27/1000, Loss: 0.669163167476654\n",
      "Epoch 28/1000, Loss: 0.6701868772506714\n",
      "Epoch 29/1000, Loss: 0.6688515543937683\n",
      "Epoch 30/1000, Loss: 0.6686007380485535\n",
      "Epoch 31/1000, Loss: 0.6682682037353516\n",
      "Epoch 32/1000, Loss: 0.6631338000297546\n",
      "Epoch 33/1000, Loss: 0.6656441688537598\n",
      "Epoch 34/1000, Loss: 0.6683602333068848\n",
      "Epoch 35/1000, Loss: 0.662071168422699\n",
      "Epoch 36/1000, Loss: 0.6662683486938477\n",
      "Epoch 37/1000, Loss: 0.6639024615287781\n",
      "Epoch 38/1000, Loss: 0.6615301370620728\n",
      "Epoch 39/1000, Loss: 0.7050246000289917\n",
      "Epoch 40/1000, Loss: 0.6581800580024719\n",
      "Epoch 41/1000, Loss: 0.657296895980835\n",
      "Epoch 42/1000, Loss: 0.6562994122505188\n",
      "Epoch 43/1000, Loss: 0.659015417098999\n",
      "Epoch 44/1000, Loss: 0.6564733982086182\n",
      "Epoch 45/1000, Loss: 0.6575620174407959\n",
      "Epoch 46/1000, Loss: 0.7073943614959717\n",
      "Epoch 47/1000, Loss: 0.7025288343429565\n",
      "Epoch 48/1000, Loss: 0.6517125368118286\n",
      "Epoch 49/1000, Loss: 0.6556825637817383\n",
      "Epoch 50/1000, Loss: 0.6508103609085083\n",
      "Epoch 51/1000, Loss: 0.6455459594726562\n",
      "Epoch 52/1000, Loss: 0.649304211139679\n",
      "Epoch 53/1000, Loss: 0.6461368799209595\n",
      "Epoch 54/1000, Loss: 0.6891048550605774\n",
      "Epoch 55/1000, Loss: 0.6449229717254639\n",
      "Epoch 56/1000, Loss: 0.6498098373413086\n",
      "Epoch 57/1000, Loss: 0.6522444486618042\n",
      "Epoch 58/1000, Loss: 0.6629887819290161\n",
      "Epoch 59/1000, Loss: 0.6584101915359497\n",
      "Epoch 60/1000, Loss: 0.6604813933372498\n",
      "Epoch 61/1000, Loss: 0.6473861932754517\n",
      "Epoch 62/1000, Loss: 0.6405700445175171\n",
      "Epoch 63/1000, Loss: 0.6424955129623413\n",
      "Epoch 64/1000, Loss: 0.64466792345047\n",
      "Epoch 65/1000, Loss: 0.6319593787193298\n",
      "Epoch 66/1000, Loss: 0.6491698622703552\n",
      "Epoch 67/1000, Loss: 0.643459677696228\n",
      "Epoch 68/1000, Loss: 0.6449874043464661\n",
      "Epoch 69/1000, Loss: 0.64690762758255\n",
      "Epoch 70/1000, Loss: 0.6491802930831909\n",
      "Epoch 71/1000, Loss: 0.6407817602157593\n",
      "Epoch 72/1000, Loss: 0.6318725347518921\n",
      "Epoch 73/1000, Loss: 0.6328735947608948\n",
      "Epoch 74/1000, Loss: 0.6393409967422485\n",
      "Epoch 75/1000, Loss: 0.6348937153816223\n",
      "Epoch 76/1000, Loss: 0.631016731262207\n",
      "Epoch 77/1000, Loss: 0.6293973922729492\n",
      "Epoch 78/1000, Loss: 0.6285020112991333\n",
      "Epoch 79/1000, Loss: 0.6196759939193726\n",
      "Epoch 80/1000, Loss: 0.6347547769546509\n",
      "Epoch 81/1000, Loss: 0.6257933378219604\n",
      "Epoch 82/1000, Loss: 0.6207525134086609\n",
      "Epoch 83/1000, Loss: 0.6155634522438049\n",
      "Epoch 84/1000, Loss: 0.6230552196502686\n",
      "Epoch 85/1000, Loss: 0.7058991193771362\n",
      "Epoch 86/1000, Loss: 0.6256515979766846\n",
      "Epoch 87/1000, Loss: 0.6293097734451294\n",
      "Epoch 88/1000, Loss: 0.6160151958465576\n",
      "Epoch 89/1000, Loss: 0.6324754953384399\n",
      "Epoch 90/1000, Loss: 0.6179811358451843\n",
      "Epoch 91/1000, Loss: 0.6125100255012512\n",
      "Epoch 92/1000, Loss: 0.6021618843078613\n",
      "Epoch 93/1000, Loss: 0.6914470195770264\n",
      "Epoch 94/1000, Loss: 0.6172106862068176\n",
      "Epoch 95/1000, Loss: 0.6140546798706055\n",
      "Epoch 96/1000, Loss: 0.6113649010658264\n",
      "Epoch 97/1000, Loss: 0.6176358461380005\n",
      "Epoch 98/1000, Loss: 0.6152230501174927\n",
      "Epoch 99/1000, Loss: 0.6214768886566162\n",
      "Epoch 100/1000, Loss: 0.6208804249763489\n",
      "Epoch 101/1000, Loss: 0.6150064468383789\n",
      "Epoch 102/1000, Loss: 0.6123658418655396\n",
      "Epoch 103/1000, Loss: 0.6115738749504089\n",
      "Epoch 104/1000, Loss: 0.602179229259491\n",
      "Epoch 105/1000, Loss: 0.6746617555618286\n",
      "Epoch 106/1000, Loss: 0.6118466854095459\n",
      "Epoch 107/1000, Loss: 0.6225130558013916\n",
      "Epoch 108/1000, Loss: 0.6090240478515625\n",
      "Epoch 109/1000, Loss: 0.6090167760848999\n",
      "Epoch 110/1000, Loss: 0.5977579355239868\n",
      "Epoch 111/1000, Loss: 0.6087989807128906\n",
      "Epoch 112/1000, Loss: 0.6319328546524048\n",
      "Epoch 113/1000, Loss: 0.5956346392631531\n",
      "Epoch 114/1000, Loss: 0.5949345827102661\n",
      "Epoch 115/1000, Loss: 0.6065743565559387\n",
      "Epoch 116/1000, Loss: 0.6120678186416626\n",
      "Epoch 117/1000, Loss: 0.6052612662315369\n",
      "Epoch 118/1000, Loss: 0.617240846157074\n",
      "Epoch 119/1000, Loss: 0.6040946245193481\n",
      "Epoch 120/1000, Loss: 0.6092526912689209\n",
      "Epoch 121/1000, Loss: 0.5900542736053467\n",
      "Epoch 122/1000, Loss: 0.5958775281906128\n",
      "Epoch 123/1000, Loss: 0.5952432751655579\n",
      "Epoch 124/1000, Loss: 0.6012321710586548\n",
      "Epoch 125/1000, Loss: 0.6006576418876648\n",
      "Epoch 126/1000, Loss: 0.5845746994018555\n",
      "Epoch 127/1000, Loss: 0.5972317457199097\n",
      "Epoch 128/1000, Loss: 0.6032580137252808\n",
      "Epoch 129/1000, Loss: 0.5984036922454834\n",
      "Epoch 130/1000, Loss: 0.6017012000083923\n",
      "Epoch 131/1000, Loss: 0.583237886428833\n",
      "Epoch 132/1000, Loss: 0.5825552940368652\n",
      "Epoch 133/1000, Loss: 0.5961426496505737\n",
      "Epoch 134/1000, Loss: 0.595575213432312\n",
      "Epoch 135/1000, Loss: 0.6022622585296631\n",
      "Epoch 136/1000, Loss: 0.594446063041687\n",
      "Epoch 137/1000, Loss: 0.5865163207054138\n",
      "Epoch 138/1000, Loss: 0.5858955383300781\n",
      "Epoch 139/1000, Loss: 0.592756450176239\n",
      "Epoch 140/1000, Loss: 0.5921965837478638\n",
      "Epoch 141/1000, Loss: 0.5840493440628052\n",
      "Epoch 142/1000, Loss: 0.5947891473770142\n",
      "Epoch 143/1000, Loss: 0.5905455350875854\n",
      "Epoch 144/1000, Loss: 0.5899897813796997\n",
      "Epoch 145/1000, Loss: 0.5816126465797424\n",
      "Epoch 146/1000, Loss: 0.6533560752868652\n",
      "Epoch 147/1000, Loss: 0.5883331894874573\n",
      "Epoch 148/1000, Loss: 0.5797977447509766\n",
      "Epoch 149/1000, Loss: 0.5872511267662048\n",
      "Epoch 150/1000, Loss: 0.5867041349411011\n",
      "Epoch 151/1000, Loss: 0.5779904723167419\n",
      "Epoch 152/1000, Loss: 0.5664072036743164\n",
      "Epoch 153/1000, Loss: 0.5905651450157166\n",
      "Epoch 154/1000, Loss: 0.592844545841217\n",
      "Epoch 155/1000, Loss: 0.6146069765090942\n",
      "Epoch 156/1000, Loss: 0.6003299951553345\n",
      "Epoch 157/1000, Loss: 0.5828934907913208\n",
      "Epoch 158/1000, Loss: 0.5823724269866943\n",
      "Epoch 159/1000, Loss: 0.5904784202575684\n",
      "Epoch 160/1000, Loss: 0.5900252461433411\n",
      "Epoch 161/1000, Loss: 0.580845057964325\n",
      "Epoch 162/1000, Loss: 0.589116096496582\n",
      "Epoch 163/1000, Loss: 0.5974961519241333\n",
      "Epoch 164/1000, Loss: 0.5526326894760132\n",
      "Epoch 165/1000, Loss: 0.5698491930961609\n",
      "Epoch 166/1000, Loss: 0.5962842702865601\n",
      "Epoch 167/1000, Loss: 0.568696916103363\n",
      "Epoch 168/1000, Loss: 0.5590047836303711\n",
      "Epoch 169/1000, Loss: 0.5767092704772949\n",
      "Epoch 170/1000, Loss: 0.5761803984642029\n",
      "Epoch 171/1000, Loss: 0.5477968454360962\n",
      "Epoch 172/1000, Loss: 0.5831356048583984\n",
      "Epoch 173/1000, Loss: 0.573101282119751\n",
      "Epoch 174/1000, Loss: 0.5551831126213074\n",
      "Epoch 175/1000, Loss: 0.5545543432235718\n",
      "Epoch 176/1000, Loss: 0.5826244354248047\n",
      "Epoch 177/1000, Loss: 0.5821670293807983\n",
      "Epoch 178/1000, Loss: 0.5623595714569092\n",
      "Epoch 179/1000, Loss: 0.5812609195709229\n",
      "Epoch 180/1000, Loss: 0.5682095885276794\n",
      "Epoch 181/1000, Loss: 0.5606857538223267\n",
      "Epoch 182/1000, Loss: 0.5667207837104797\n",
      "Epoch 183/1000, Loss: 0.5396760702133179\n",
      "Epoch 184/1000, Loss: 0.5490015745162964\n",
      "Epoch 185/1000, Loss: 0.5584273338317871\n",
      "Epoch 186/1000, Loss: 0.5578483939170837\n",
      "Epoch 187/1000, Loss: 0.5674577355384827\n",
      "Epoch 188/1000, Loss: 0.5669485330581665\n",
      "Epoch 189/1000, Loss: 0.5720807313919067\n",
      "Epoch 190/1000, Loss: 0.5556026697158813\n",
      "Epoch 191/1000, Loss: 0.5758534669876099\n",
      "Epoch 192/1000, Loss: 0.5440487265586853\n",
      "Epoch 193/1000, Loss: 0.5539597868919373\n",
      "Epoch 194/1000, Loss: 0.6062428951263428\n",
      "Epoch 195/1000, Loss: 0.542252779006958\n",
      "Epoch 196/1000, Loss: 0.5523247718811035\n",
      "Epoch 197/1000, Loss: 0.5625042915344238\n",
      "Epoch 198/1000, Loss: 0.5449091196060181\n",
      "Epoch 199/1000, Loss: 0.5658846497535706\n",
      "Epoch 200/1000, Loss: 0.5761716961860657\n",
      "Epoch 201/1000, Loss: 0.5537053346633911\n",
      "Epoch 202/1000, Loss: 0.5490398406982422\n",
      "Epoch 203/1000, Loss: 0.5374401807785034\n",
      "Epoch 204/1000, Loss: 0.5406308174133301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/1000, Loss: 0.5474289655685425\n",
      "Epoch 206/1000, Loss: 0.6148419380187988\n",
      "Epoch 207/1000, Loss: 0.5463526248931885\n",
      "Epoch 208/1000, Loss: 0.6235527992248535\n",
      "Epoch 209/1000, Loss: 0.5452894568443298\n",
      "Epoch 210/1000, Loss: 0.521877646446228\n",
      "Epoch 211/1000, Loss: 0.5442463755607605\n",
      "Epoch 212/1000, Loss: 0.5714670419692993\n",
      "Epoch 213/1000, Loss: 0.5316013097763062\n",
      "Epoch 214/1000, Loss: 0.5660117864608765\n",
      "Epoch 215/1000, Loss: 0.5707005858421326\n",
      "Epoch 216/1000, Loss: 0.5299090147018433\n",
      "Epoch 217/1000, Loss: 0.5293529629707336\n",
      "Epoch 218/1000, Loss: 0.5169225931167603\n",
      "Epoch 219/1000, Loss: 0.5401463508605957\n",
      "Epoch 220/1000, Loss: 0.5516031980514526\n",
      "Epoch 221/1000, Loss: 0.5912490487098694\n",
      "Epoch 222/1000, Loss: 0.5385971069335938\n",
      "Epoch 223/1000, Loss: 0.562371015548706\n",
      "Epoch 224/1000, Loss: 0.5619690418243408\n",
      "Epoch 225/1000, Loss: 0.5248239040374756\n",
      "Epoch 226/1000, Loss: 0.5242730379104614\n",
      "Epoch 227/1000, Loss: 0.5237203240394592\n",
      "Epoch 228/1000, Loss: 0.5196224451065063\n",
      "Epoch 229/1000, Loss: 0.5226035118103027\n",
      "Epoch 230/1000, Loss: 0.5437942147254944\n",
      "Epoch 231/1000, Loss: 0.5466176271438599\n",
      "Epoch 232/1000, Loss: 0.5209198594093323\n",
      "Epoch 233/1000, Loss: 0.5457180142402649\n",
      "Epoch 234/1000, Loss: 0.5070887804031372\n",
      "Epoch 235/1000, Loss: 0.5192705988883972\n",
      "Epoch 236/1000, Loss: 0.5406622290611267\n",
      "Epoch 237/1000, Loss: 0.5271826386451721\n",
      "Epoch 238/1000, Loss: 0.5046752691268921\n",
      "Epoch 239/1000, Loss: 0.5690299272537231\n",
      "Epoch 240/1000, Loss: 0.5425810813903809\n",
      "Epoch 241/1000, Loss: 0.5636414289474487\n",
      "Epoch 242/1000, Loss: 0.502217173576355\n",
      "Epoch 243/1000, Loss: 0.5148236751556396\n",
      "Epoch 244/1000, Loss: 0.5142670273780823\n",
      "Epoch 245/1000, Loss: 0.5803036093711853\n",
      "Epoch 246/1000, Loss: 0.5073506832122803\n",
      "Epoch 247/1000, Loss: 0.5126038193702698\n",
      "Epoch 248/1000, Loss: 0.4985652565956116\n",
      "Epoch 249/1000, Loss: 0.538557767868042\n",
      "Epoch 250/1000, Loss: 0.5109303593635559\n",
      "Epoch 251/1000, Loss: 0.5513031482696533\n",
      "Epoch 252/1000, Loss: 0.523509681224823\n",
      "Epoch 253/1000, Loss: 0.509255051612854\n",
      "Epoch 254/1000, Loss: 0.5363222360610962\n",
      "Epoch 255/1000, Loss: 0.5563777089118958\n",
      "Epoch 256/1000, Loss: 0.5215342044830322\n",
      "Epoch 257/1000, Loss: 0.5727206468582153\n",
      "Epoch 258/1000, Loss: 0.4925347864627838\n",
      "Epoch 259/1000, Loss: 0.5060254335403442\n",
      "Epoch 260/1000, Loss: 0.533743679523468\n",
      "Epoch 261/1000, Loss: 0.5474915504455566\n",
      "Epoch 262/1000, Loss: 0.5471146106719971\n",
      "Epoch 263/1000, Loss: 0.546743631362915\n",
      "Epoch 264/1000, Loss: 0.5177008509635925\n",
      "Epoch 265/1000, Loss: 0.5316213369369507\n",
      "Epoch 266/1000, Loss: 0.5023260712623596\n",
      "Epoch 267/1000, Loss: 0.5373722910881042\n",
      "Epoch 268/1000, Loss: 0.5223318934440613\n",
      "Epoch 269/1000, Loss: 0.492559015750885\n",
      "Epoch 270/1000, Loss: 0.5148960947990417\n",
      "Epoch 271/1000, Loss: 0.49972015619277954\n",
      "Epoch 272/1000, Loss: 0.5287032127380371\n",
      "Epoch 273/1000, Loss: 0.5282853841781616\n",
      "Epoch 274/1000, Loss: 0.513015866279602\n",
      "Epoch 275/1000, Loss: 0.5480597615242004\n",
      "Epoch 276/1000, Loss: 0.4821852147579193\n",
      "Epoch 277/1000, Loss: 0.5717036724090576\n",
      "Epoch 278/1000, Loss: 0.5413287878036499\n",
      "Epoch 279/1000, Loss: 0.5258766412734985\n",
      "Epoch 280/1000, Loss: 0.5406530499458313\n",
      "Epoch 281/1000, Loss: 0.530019998550415\n",
      "Epoch 282/1000, Loss: 0.5247255563735962\n",
      "Epoch 283/1000, Loss: 0.5445975065231323\n",
      "Epoch 284/1000, Loss: 0.5700340867042542\n",
      "Epoch 285/1000, Loss: 0.4822922348976135\n",
      "Epoch 286/1000, Loss: 0.5231956243515015\n",
      "Epoch 287/1000, Loss: 0.5227991938591003\n",
      "Epoch 288/1000, Loss: 0.4912833869457245\n",
      "Epoch 289/1000, Loss: 0.5220035314559937\n",
      "Epoch 290/1000, Loss: 0.5216053128242493\n",
      "Epoch 291/1000, Loss: 0.5054956078529358\n",
      "Epoch 292/1000, Loss: 0.505052924156189\n",
      "Epoch 293/1000, Loss: 0.5604140758514404\n",
      "Epoch 294/1000, Loss: 0.510112464427948\n",
      "Epoch 295/1000, Loss: 0.48780590295791626\n",
      "Epoch 296/1000, Loss: 0.48730096220970154\n",
      "Epoch 297/1000, Loss: 0.5188418626785278\n",
      "Epoch 298/1000, Loss: 0.5089448690414429\n",
      "Epoch 299/1000, Loss: 0.5180404782295227\n",
      "Epoch 300/1000, Loss: 0.5080903172492981\n",
      "Epoch 301/1000, Loss: 0.5172632932662964\n",
      "Epoch 302/1000, Loss: 0.5331562161445618\n",
      "Epoch 303/1000, Loss: 0.48380959033966064\n",
      "Epoch 304/1000, Loss: 0.47360092401504517\n",
      "Epoch 305/1000, Loss: 0.49926212430000305\n",
      "Epoch 306/1000, Loss: 0.46583929657936096\n",
      "Epoch 307/1000, Loss: 0.4886012673377991\n",
      "Epoch 308/1000, Loss: 0.5046776533126831\n",
      "Epoch 309/1000, Loss: 0.49750185012817383\n",
      "Epoch 310/1000, Loss: 0.497059166431427\n",
      "Epoch 311/1000, Loss: 0.49661725759506226\n",
      "Epoch 312/1000, Loss: 0.5312429666519165\n",
      "Epoch 313/1000, Loss: 0.5294501781463623\n",
      "Epoch 314/1000, Loss: 0.4953102469444275\n",
      "Epoch 315/1000, Loss: 0.4779263734817505\n",
      "Epoch 316/1000, Loss: 0.5182102918624878\n",
      "Epoch 317/1000, Loss: 0.5281424522399902\n",
      "Epoch 318/1000, Loss: 0.493600070476532\n",
      "Epoch 319/1000, Loss: 0.5103390216827393\n",
      "Epoch 320/1000, Loss: 0.4755437970161438\n",
      "Epoch 321/1000, Loss: 0.4989309310913086\n",
      "Epoch 322/1000, Loss: 0.4984379708766937\n",
      "Epoch 323/1000, Loss: 0.4805534780025482\n",
      "Epoch 324/1000, Loss: 0.508463978767395\n",
      "Epoch 325/1000, Loss: 0.4794020652770996\n",
      "Epoch 326/1000, Loss: 0.5313734412193298\n",
      "Epoch 327/1000, Loss: 0.543676495552063\n",
      "Epoch 328/1000, Loss: 0.5130865573883057\n",
      "Epoch 329/1000, Loss: 0.5066962242126465\n",
      "Epoch 330/1000, Loss: 0.5063520669937134\n",
      "Epoch 331/1000, Loss: 0.4705020785331726\n",
      "Epoch 332/1000, Loss: 0.5056630373001099\n",
      "Epoch 333/1000, Loss: 0.5410153269767761\n",
      "Epoch 334/1000, Loss: 0.4512674808502197\n",
      "Epoch 335/1000, Loss: 0.4866587817668915\n",
      "Epoch 336/1000, Loss: 0.4502560496330261\n",
      "Epoch 337/1000, Loss: 0.46779268980026245\n",
      "Epoch 338/1000, Loss: 0.4492264986038208\n",
      "Epoch 339/1000, Loss: 0.5213049650192261\n",
      "Epoch 340/1000, Loss: 0.47218844294548035\n",
      "Epoch 341/1000, Loss: 0.5024231672286987\n",
      "Epoch 342/1000, Loss: 0.46545422077178955\n",
      "Epoch 343/1000, Loss: 0.4833473265171051\n",
      "Epoch 344/1000, Loss: 0.4883810877799988\n",
      "Epoch 345/1000, Loss: 0.4640721082687378\n",
      "Epoch 346/1000, Loss: 0.5006275177001953\n",
      "Epoch 347/1000, Loss: 0.4686368703842163\n",
      "Epoch 348/1000, Loss: 0.49993014335632324\n",
      "Epoch 349/1000, Loss: 0.48092740774154663\n",
      "Epoch 350/1000, Loss: 0.4618269205093384\n",
      "Epoch 351/1000, Loss: 0.44262564182281494\n",
      "Epoch 352/1000, Loss: 0.47973453998565674\n",
      "Epoch 353/1000, Loss: 0.4793371558189392\n",
      "Epoch 354/1000, Loss: 0.5031996369361877\n",
      "Epoch 355/1000, Loss: 0.49749869108200073\n",
      "Epoch 356/1000, Loss: 0.47814780473709106\n",
      "Epoch 357/1000, Loss: 0.49680617451667786\n",
      "Epoch 358/1000, Loss: 0.5537734031677246\n",
      "Epoch 359/1000, Loss: 0.4961155354976654\n",
      "Epoch 360/1000, Loss: 0.4765641987323761\n",
      "Epoch 361/1000, Loss: 0.5146714448928833\n",
      "Epoch 362/1000, Loss: 0.4950689673423767\n",
      "Epoch 363/1000, Loss: 0.49957185983657837\n",
      "Epoch 364/1000, Loss: 0.47498688101768494\n",
      "Epoch 365/1000, Loss: 0.4792903661727905\n",
      "Epoch 366/1000, Loss: 0.47421735525131226\n",
      "Epoch 367/1000, Loss: 0.4738331735134125\n",
      "Epoch 368/1000, Loss: 0.4930539131164551\n",
      "Epoch 369/1000, Loss: 0.45343464612960815\n",
      "Epoch 370/1000, Loss: 0.4530183970928192\n",
      "Epoch 371/1000, Loss: 0.45259982347488403\n",
      "Epoch 372/1000, Loss: 0.5115512013435364\n",
      "Epoch 373/1000, Loss: 0.4914321303367615\n",
      "Epoch 374/1000, Loss: 0.4911016821861267\n",
      "Epoch 375/1000, Loss: 0.4309595823287964\n",
      "Epoch 376/1000, Loss: 0.5104241371154785\n",
      "Epoch 377/1000, Loss: 0.45431455969810486\n",
      "Epoch 378/1000, Loss: 0.5737935900688171\n",
      "Epoch 379/1000, Loss: 0.5095782279968262\n",
      "Epoch 380/1000, Loss: 0.4689369797706604\n",
      "Epoch 381/1000, Loss: 0.42811644077301025\n",
      "Epoch 382/1000, Loss: 0.43215394020080566\n",
      "Epoch 383/1000, Loss: 0.4881874918937683\n",
      "Epoch 384/1000, Loss: 0.43124520778656006\n",
      "Epoch 385/1000, Loss: 0.48754602670669556\n",
      "Epoch 386/1000, Loss: 0.48723071813583374\n",
      "Epoch 387/1000, Loss: 0.44588688015937805\n",
      "Epoch 388/1000, Loss: 0.4866087734699249\n",
      "Epoch 389/1000, Loss: 0.4863017797470093\n",
      "Epoch 390/1000, Loss: 0.48599162697792053\n",
      "Epoch 391/1000, Loss: 0.4668874740600586\n",
      "Epoch 392/1000, Loss: 0.46896982192993164\n",
      "Epoch 393/1000, Loss: 0.5266637206077576\n",
      "Epoch 394/1000, Loss: 0.46840524673461914\n",
      "Epoch 395/1000, Loss: 0.4680720865726471\n",
      "Epoch 396/1000, Loss: 0.4841240644454956\n",
      "Epoch 397/1000, Loss: 0.48380744457244873\n",
      "Epoch 398/1000, Loss: 0.4879601001739502\n",
      "Epoch 399/1000, Loss: 0.4875788688659668\n",
      "Epoch 400/1000, Loss: 0.4828600287437439\n",
      "Epoch 401/1000, Loss: 0.48254865407943726\n",
      "Epoch 402/1000, Loss: 0.43978002667427063\n",
      "Epoch 403/1000, Loss: 0.42211252450942993\n",
      "Epoch 404/1000, Loss: 0.44290149211883545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405/1000, Loss: 0.4385848045349121\n",
      "Epoch 406/1000, Loss: 0.523872971534729\n",
      "Epoch 407/1000, Loss: 0.48071548342704773\n",
      "Epoch 408/1000, Loss: 0.4804120659828186\n",
      "Epoch 409/1000, Loss: 0.4585484564304352\n",
      "Epoch 410/1000, Loss: 0.47981852293014526\n",
      "Epoch 411/1000, Loss: 0.4578625559806824\n",
      "Epoch 412/1000, Loss: 0.46093112230300903\n",
      "Epoch 413/1000, Loss: 0.4354279041290283\n",
      "Epoch 414/1000, Loss: 0.478633850812912\n",
      "Epoch 415/1000, Loss: 0.4564962387084961\n",
      "Epoch 416/1000, Loss: 0.4780540466308594\n",
      "Epoch 417/1000, Loss: 0.499705046415329\n",
      "Epoch 418/1000, Loss: 0.45549875497817993\n",
      "Epoch 419/1000, Loss: 0.5028398036956787\n",
      "Epoch 420/1000, Loss: 0.4327566623687744\n",
      "Epoch 421/1000, Loss: 0.49873974919319153\n",
      "Epoch 422/1000, Loss: 0.5223097801208496\n",
      "Epoch 423/1000, Loss: 0.43543297052383423\n",
      "Epoch 424/1000, Loss: 0.479671835899353\n",
      "Epoch 425/1000, Loss: 0.4571399390697479\n",
      "Epoch 426/1000, Loss: 0.45683494210243225\n",
      "Epoch 427/1000, Loss: 0.4341081380844116\n",
      "Epoch 428/1000, Loss: 0.42974868416786194\n",
      "Epoch 429/1000, Loss: 0.45579051971435547\n",
      "Epoch 430/1000, Loss: 0.4740622639656067\n",
      "Epoch 431/1000, Loss: 0.49635785818099976\n",
      "Epoch 432/1000, Loss: 0.4508592486381531\n",
      "Epoch 433/1000, Loss: 0.4543212950229645\n",
      "Epoch 434/1000, Loss: 0.49563169479370117\n",
      "Epoch 435/1000, Loss: 0.4535713195800781\n",
      "Epoch 436/1000, Loss: 0.4719359278678894\n",
      "Epoch 437/1000, Loss: 0.3806071877479553\n",
      "Epoch 438/1000, Loss: 0.49468114972114563\n",
      "Epoch 439/1000, Loss: 0.5174005627632141\n",
      "Epoch 440/1000, Loss: 0.4712001085281372\n",
      "Epoch 441/1000, Loss: 0.5170190334320068\n",
      "Epoch 442/1000, Loss: 0.4244336485862732\n",
      "Epoch 443/1000, Loss: 0.5166454911231995\n",
      "Epoch 444/1000, Loss: 0.4005093574523926\n",
      "Epoch 445/1000, Loss: 0.4274202585220337\n",
      "Epoch 446/1000, Loss: 0.44623368978500366\n",
      "Epoch 447/1000, Loss: 0.39923879504203796\n",
      "Epoch 448/1000, Loss: 0.4923330545425415\n",
      "Epoch 449/1000, Loss: 0.4726520776748657\n",
      "Epoch 450/1000, Loss: 0.44493216276168823\n",
      "Epoch 451/1000, Loss: 0.46812331676483154\n",
      "Epoch 452/1000, Loss: 0.42071646451950073\n",
      "Epoch 453/1000, Loss: 0.42035287618637085\n",
      "Epoch 454/1000, Loss: 0.5382761359214783\n",
      "Epoch 455/1000, Loss: 0.44703441858291626\n",
      "Epoch 456/1000, Loss: 0.5617627501487732\n",
      "Epoch 457/1000, Loss: 0.41893061995506287\n",
      "Epoch 458/1000, Loss: 0.4220682382583618\n",
      "Epoch 459/1000, Loss: 0.4694027006626129\n",
      "Epoch 460/1000, Loss: 0.4896659553050995\n",
      "Epoch 461/1000, Loss: 0.4687398076057434\n",
      "Epoch 462/1000, Loss: 0.39321306347846985\n",
      "Epoch 463/1000, Loss: 0.5162069201469421\n",
      "Epoch 464/1000, Loss: 0.4647265076637268\n",
      "Epoch 465/1000, Loss: 0.4161887466907501\n",
      "Epoch 466/1000, Loss: 0.46420690417289734\n",
      "Epoch 467/1000, Loss: 0.41547995805740356\n",
      "Epoch 468/1000, Loss: 0.4393939971923828\n",
      "Epoch 469/1000, Loss: 0.41474810242652893\n",
      "Epoch 470/1000, Loss: 0.41438519954681396\n",
      "Epoch 471/1000, Loss: 0.4384452700614929\n",
      "Epoch 472/1000, Loss: 0.48706671595573425\n",
      "Epoch 473/1000, Loss: 0.4657728672027588\n",
      "Epoch 474/1000, Loss: 0.4620797336101532\n",
      "Epoch 475/1000, Loss: 0.486421138048172\n",
      "Epoch 476/1000, Loss: 0.48232659697532654\n",
      "Epoch 477/1000, Loss: 0.4155030846595764\n",
      "Epoch 478/1000, Loss: 0.3621153235435486\n",
      "Epoch 479/1000, Loss: 0.4855830669403076\n",
      "Epoch 480/1000, Loss: 0.41089117527008057\n",
      "Epoch 481/1000, Loss: 0.43541979789733887\n",
      "Epoch 482/1000, Loss: 0.46394768357276917\n",
      "Epoch 483/1000, Loss: 0.3599519729614258\n",
      "Epoch 484/1000, Loss: 0.4845471680164337\n",
      "Epoch 485/1000, Loss: 0.43817296624183655\n",
      "Epoch 486/1000, Loss: 0.4339599311351776\n",
      "Epoch 487/1000, Loss: 0.44977158308029175\n",
      "Epoch 488/1000, Loss: 0.4585704505443573\n",
      "Epoch 489/1000, Loss: 0.5087766647338867\n",
      "Epoch 490/1000, Loss: 0.5086253881454468\n",
      "Epoch 491/1000, Loss: 0.4831698536872864\n",
      "Epoch 492/1000, Loss: 0.4576314687728882\n",
      "Epoch 493/1000, Loss: 0.48754629492759705\n",
      "Epoch 494/1000, Loss: 0.4062919318675995\n",
      "Epoch 495/1000, Loss: 0.5333601236343384\n",
      "Epoch 496/1000, Loss: 0.431167334318161\n",
      "Epoch 497/1000, Loss: 0.4053220748901367\n",
      "Epoch 498/1000, Loss: 0.4818156361579895\n",
      "Epoch 499/1000, Loss: 0.4816153645515442\n",
      "Epoch 500/1000, Loss: 0.43002235889434814\n",
      "Epoch 501/1000, Loss: 0.4039854407310486\n",
      "Epoch 502/1000, Loss: 0.48100319504737854\n",
      "Epoch 503/1000, Loss: 0.42912185192108154\n",
      "Epoch 504/1000, Loss: 0.4288285970687866\n",
      "Epoch 505/1000, Loss: 0.4333994388580322\n",
      "Epoch 506/1000, Loss: 0.4542251527309418\n",
      "Epoch 507/1000, Loss: 0.45398586988449097\n",
      "Epoch 508/1000, Loss: 0.4016314744949341\n",
      "Epoch 509/1000, Loss: 0.40131399035453796\n",
      "Epoch 510/1000, Loss: 0.42714250087738037\n",
      "Epoch 511/1000, Loss: 0.37449491024017334\n",
      "Epoch 512/1000, Loss: 0.48433351516723633\n",
      "Epoch 513/1000, Loss: 0.4263242483139038\n",
      "Epoch 514/1000, Loss: 0.37864255905151367\n",
      "Epoch 515/1000, Loss: 0.40458646416664124\n",
      "Epoch 516/1000, Loss: 0.45189911127090454\n",
      "Epoch 517/1000, Loss: 0.3723050653934479\n",
      "Epoch 518/1000, Loss: 0.39842551946640015\n",
      "Epoch 519/1000, Loss: 0.4295528829097748\n",
      "Epoch 520/1000, Loss: 0.42435550689697266\n",
      "Epoch 521/1000, Loss: 0.3974427878856659\n",
      "Epoch 522/1000, Loss: 0.3485087752342224\n",
      "Epoch 523/1000, Loss: 0.39678364992141724\n",
      "Epoch 524/1000, Loss: 0.4499841630458832\n",
      "Epoch 525/1000, Loss: 0.4277547001838684\n",
      "Epoch 526/1000, Loss: 0.39579737186431885\n",
      "Epoch 527/1000, Loss: 0.3685786724090576\n",
      "Epoch 528/1000, Loss: 0.5029483437538147\n",
      "Epoch 529/1000, Loss: 0.4287320375442505\n",
      "Epoch 530/1000, Loss: 0.45344752073287964\n",
      "Epoch 531/1000, Loss: 0.47546523809432983\n",
      "Epoch 532/1000, Loss: 0.44816717505455017\n",
      "Epoch 533/1000, Loss: 0.47510242462158203\n",
      "Epoch 534/1000, Loss: 0.44771814346313477\n",
      "Epoch 535/1000, Loss: 0.44749805331230164\n",
      "Epoch 536/1000, Loss: 0.5074668526649475\n",
      "Epoch 537/1000, Loss: 0.4518432021141052\n",
      "Epoch 538/1000, Loss: 0.41946887969970703\n",
      "Epoch 539/1000, Loss: 0.44663381576538086\n",
      "Epoch 540/1000, Loss: 0.4518947899341583\n",
      "Epoch 541/1000, Loss: 0.4474940598011017\n",
      "Epoch 542/1000, Loss: 0.39091312885284424\n",
      "Epoch 543/1000, Loss: 0.44578319787979126\n",
      "Epoch 544/1000, Loss: 0.4239746630191803\n",
      "Epoch 545/1000, Loss: 0.4453532099723816\n",
      "Epoch 546/1000, Loss: 0.3342922329902649\n",
      "Epoch 547/1000, Loss: 0.38941311836242676\n",
      "Epoch 548/1000, Loss: 0.416908323764801\n",
      "Epoch 549/1000, Loss: 0.45052388310432434\n",
      "Epoch 550/1000, Loss: 0.47813159227371216\n",
      "Epoch 551/1000, Loss: 0.41613611578941345\n",
      "Epoch 552/1000, Loss: 0.3599472939968109\n",
      "Epoch 553/1000, Loss: 0.4436337351799011\n",
      "Epoch 554/1000, Loss: 0.3929755985736847\n",
      "Epoch 555/1000, Loss: 0.3870099186897278\n",
      "Epoch 556/1000, Loss: 0.5047489404678345\n",
      "Epoch 557/1000, Loss: 0.4709591865539551\n",
      "Epoch 558/1000, Loss: 0.41433703899383545\n",
      "Epoch 559/1000, Loss: 0.41408097743988037\n",
      "Epoch 560/1000, Loss: 0.4189304709434509\n",
      "Epoch 561/1000, Loss: 0.39024198055267334\n",
      "Epoch 562/1000, Loss: 0.3565264940261841\n",
      "Epoch 563/1000, Loss: 0.4415060579776764\n",
      "Epoch 564/1000, Loss: 0.44129350781440735\n",
      "Epoch 565/1000, Loss: 0.4172692894935608\n",
      "Epoch 566/1000, Loss: 0.38835737109184265\n",
      "Epoch 567/1000, Loss: 0.4165820777416229\n",
      "Epoch 568/1000, Loss: 0.38310855627059937\n",
      "Epoch 569/1000, Loss: 0.3541085422039032\n",
      "Epoch 570/1000, Loss: 0.3884096145629883\n",
      "Epoch 571/1000, Loss: 0.4685894548892975\n",
      "Epoch 572/1000, Loss: 0.43959158658981323\n",
      "Epoch 573/1000, Loss: 0.41051536798477173\n",
      "Epoch 574/1000, Loss: 0.4731729030609131\n",
      "Epoch 575/1000, Loss: 0.4390014410018921\n",
      "Epoch 576/1000, Loss: 0.3228151202201843\n",
      "Epoch 577/1000, Loss: 0.4386085271835327\n",
      "Epoch 578/1000, Loss: 0.4674893021583557\n",
      "Epoch 579/1000, Loss: 0.43820780515670776\n",
      "Epoch 580/1000, Loss: 0.5019335150718689\n",
      "Epoch 581/1000, Loss: 0.3850025236606598\n",
      "Epoch 582/1000, Loss: 0.437637597322464\n",
      "Epoch 583/1000, Loss: 0.466733455657959\n",
      "Epoch 584/1000, Loss: 0.46658429503440857\n",
      "Epoch 585/1000, Loss: 0.37834328413009644\n",
      "Epoch 586/1000, Loss: 0.4129675328731537\n",
      "Epoch 587/1000, Loss: 0.46613630652427673\n",
      "Epoch 588/1000, Loss: 0.4124017655849457\n",
      "Epoch 589/1000, Loss: 0.4658394157886505\n",
      "Epoch 590/1000, Loss: 0.41179949045181274\n",
      "Epoch 591/1000, Loss: 0.49515461921691895\n",
      "Epoch 592/1000, Loss: 0.37647533416748047\n",
      "Epoch 593/1000, Loss: 0.31685131788253784\n",
      "Epoch 594/1000, Loss: 0.4651130437850952\n",
      "Epoch 595/1000, Loss: 0.375669926404953\n",
      "Epoch 596/1000, Loss: 0.37539640069007874\n",
      "Epoch 597/1000, Loss: 0.434814453125\n",
      "Epoch 598/1000, Loss: 0.3449609875679016\n",
      "Epoch 599/1000, Loss: 0.37457558512687683\n",
      "Epoch 600/1000, Loss: 0.4941840171813965\n",
      "Epoch 601/1000, Loss: 0.4040359556674957\n",
      "Epoch 602/1000, Loss: 0.43500426411628723\n",
      "Epoch 603/1000, Loss: 0.43366390466690063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 604/1000, Loss: 0.433470219373703\n",
      "Epoch 605/1000, Loss: 0.43327978253364563\n",
      "Epoch 606/1000, Loss: 0.46331337094306946\n",
      "Epoch 607/1000, Loss: 0.372374951839447\n",
      "Epoch 608/1000, Loss: 0.46788427233695984\n",
      "Epoch 609/1000, Loss: 0.40218955278396606\n",
      "Epoch 610/1000, Loss: 0.4323505759239197\n",
      "Epoch 611/1000, Loss: 0.4017390012741089\n",
      "Epoch 612/1000, Loss: 0.3453066945075989\n",
      "Epoch 613/1000, Loss: 0.3708004355430603\n",
      "Epoch 614/1000, Loss: 0.40108251571655273\n",
      "Epoch 615/1000, Loss: 0.43602800369262695\n",
      "Epoch 616/1000, Loss: 0.4357999563217163\n",
      "Epoch 617/1000, Loss: 0.36976587772369385\n",
      "Epoch 618/1000, Loss: 0.400221586227417\n",
      "Epoch 619/1000, Loss: 0.430752158164978\n",
      "Epoch 620/1000, Loss: 0.4613608121871948\n",
      "Epoch 621/1000, Loss: 0.36875981092453003\n",
      "Epoch 622/1000, Loss: 0.3685121238231659\n",
      "Epoch 623/1000, Loss: 0.3729151487350464\n",
      "Epoch 624/1000, Loss: 0.49177086353302\n",
      "Epoch 625/1000, Loss: 0.42972609400749207\n",
      "Epoch 626/1000, Loss: 0.42956212162971497\n",
      "Epoch 627/1000, Loss: 0.3983519971370697\n",
      "Epoch 628/1000, Loss: 0.3981524705886841\n",
      "Epoch 629/1000, Loss: 0.39794790744781494\n",
      "Epoch 630/1000, Loss: 0.3665764331817627\n",
      "Epoch 631/1000, Loss: 0.45993512868881226\n",
      "Epoch 632/1000, Loss: 0.42855584621429443\n",
      "Epoch 633/1000, Loss: 0.3658139705657959\n",
      "Epoch 634/1000, Loss: 0.3968847990036011\n",
      "Epoch 635/1000, Loss: 0.42803433537483215\n",
      "Epoch 636/1000, Loss: 0.4953853189945221\n",
      "Epoch 637/1000, Loss: 0.45912832021713257\n",
      "Epoch 638/1000, Loss: 0.3960205316543579\n",
      "Epoch 639/1000, Loss: 0.3932766318321228\n",
      "Epoch 640/1000, Loss: 0.33244940638542175\n",
      "Epoch 641/1000, Loss: 0.4539909064769745\n",
      "Epoch 642/1000, Loss: 0.39516013860702515\n",
      "Epoch 643/1000, Loss: 0.39494919776916504\n",
      "Epoch 644/1000, Loss: 0.3630080819129944\n",
      "Epoch 645/1000, Loss: 0.3627505600452423\n",
      "Epoch 646/1000, Loss: 0.33067554235458374\n",
      "Epoch 647/1000, Loss: 0.42594581842422485\n",
      "Epoch 648/1000, Loss: 0.3685128092765808\n",
      "Epoch 649/1000, Loss: 0.4321744441986084\n",
      "Epoch 650/1000, Loss: 0.48939573764801025\n",
      "Epoch 651/1000, Loss: 0.36123502254486084\n",
      "Epoch 652/1000, Loss: 0.3930535912513733\n",
      "Epoch 653/1000, Loss: 0.42495062947273254\n",
      "Epoch 654/1000, Loss: 0.4316578805446625\n",
      "Epoch 655/1000, Loss: 0.4246353209018707\n",
      "Epoch 656/1000, Loss: 0.4244747757911682\n",
      "Epoch 657/1000, Loss: 0.39891552925109863\n",
      "Epoch 658/1000, Loss: 0.39868247509002686\n",
      "Epoch 659/1000, Loss: 0.4240015149116516\n",
      "Epoch 660/1000, Loss: 0.3914823532104492\n",
      "Epoch 661/1000, Loss: 0.3912895917892456\n",
      "Epoch 662/1000, Loss: 0.3911011219024658\n",
      "Epoch 663/1000, Loss: 0.45585641264915466\n",
      "Epoch 664/1000, Loss: 0.41084858775138855\n",
      "Epoch 665/1000, Loss: 0.357974112033844\n",
      "Epoch 666/1000, Loss: 0.4555012285709381\n",
      "Epoch 667/1000, Loss: 0.39654844999313354\n",
      "Epoch 668/1000, Loss: 0.35727715492248535\n",
      "Epoch 669/1000, Loss: 0.35704344511032104\n",
      "Epoch 670/1000, Loss: 0.48777198791503906\n",
      "Epoch 671/1000, Loss: 0.454911470413208\n",
      "Epoch 672/1000, Loss: 0.36263757944107056\n",
      "Epoch 673/1000, Loss: 0.3952189087867737\n",
      "Epoch 674/1000, Loss: 0.42167362570762634\n",
      "Epoch 675/1000, Loss: 0.42151790857315063\n",
      "Epoch 676/1000, Loss: 0.4273994266986847\n",
      "Epoch 677/1000, Loss: 0.3551871180534363\n",
      "Epoch 678/1000, Loss: 0.4210497736930847\n",
      "Epoch 679/1000, Loss: 0.35471946001052856\n",
      "Epoch 680/1000, Loss: 0.3876098096370697\n",
      "Epoch 681/1000, Loss: 0.45374545454978943\n",
      "Epoch 682/1000, Loss: 0.3208116292953491\n",
      "Epoch 683/1000, Loss: 0.39269307255744934\n",
      "Epoch 684/1000, Loss: 0.3535462021827698\n",
      "Epoch 685/1000, Loss: 0.4199548363685608\n",
      "Epoch 686/1000, Loss: 0.4865252673625946\n",
      "Epoch 687/1000, Loss: 0.3194381892681122\n",
      "Epoch 688/1000, Loss: 0.38604462146759033\n",
      "Epoch 689/1000, Loss: 0.4250609278678894\n",
      "Epoch 690/1000, Loss: 0.4526914060115814\n",
      "Epoch 691/1000, Loss: 0.41900864243507385\n",
      "Epoch 692/1000, Loss: 0.3852492570877075\n",
      "Epoch 693/1000, Loss: 0.3514193296432495\n",
      "Epoch 694/1000, Loss: 0.4922237992286682\n",
      "Epoch 695/1000, Loss: 0.3846897780895233\n",
      "Epoch 696/1000, Loss: 0.3507443964481354\n",
      "Epoch 697/1000, Loss: 0.35051819682121277\n",
      "Epoch 698/1000, Loss: 0.39079344272613525\n",
      "Epoch 699/1000, Loss: 0.3906790614128113\n",
      "Epoch 700/1000, Loss: 0.3498249650001526\n",
      "Epoch 701/1000, Loss: 0.3156404197216034\n",
      "Epoch 702/1000, Loss: 0.34937113523483276\n",
      "Epoch 703/1000, Loss: 0.3831765353679657\n",
      "Epoch 704/1000, Loss: 0.3489183187484741\n",
      "Epoch 705/1000, Loss: 0.3990806043148041\n",
      "Epoch 706/1000, Loss: 0.38261330127716064\n",
      "Epoch 707/1000, Loss: 0.3824233114719391\n",
      "Epoch 708/1000, Loss: 0.42354533076286316\n",
      "Epoch 709/1000, Loss: 0.38205596804618835\n",
      "Epoch 710/1000, Loss: 0.3818748891353607\n",
      "Epoch 711/1000, Loss: 0.34735429286956787\n",
      "Epoch 712/1000, Loss: 0.38152116537094116\n",
      "Epoch 713/1000, Loss: 0.4157620370388031\n",
      "Epoch 714/1000, Loss: 0.35376280546188354\n",
      "Epoch 715/1000, Loss: 0.3879972994327545\n",
      "Epoch 716/1000, Loss: 0.3808273673057556\n",
      "Epoch 717/1000, Loss: 0.41521695256233215\n",
      "Epoch 718/1000, Loss: 0.45665243268013\n",
      "Epoch 719/1000, Loss: 0.41493791341781616\n",
      "Epoch 720/1000, Loss: 0.38012462854385376\n",
      "Epoch 721/1000, Loss: 0.4493800103664398\n",
      "Epoch 722/1000, Loss: 0.41453152894973755\n",
      "Epoch 723/1000, Loss: 0.37961000204086304\n",
      "Epoch 724/1000, Loss: 0.3446168303489685\n",
      "Epoch 725/1000, Loss: 0.3513130843639374\n",
      "Epoch 726/1000, Loss: 0.37909793853759766\n",
      "Epoch 727/1000, Loss: 0.3789278268814087\n",
      "Epoch 728/1000, Loss: 0.35059207677841187\n",
      "Epoch 729/1000, Loss: 0.3944253921508789\n",
      "Epoch 730/1000, Loss: 0.41345447301864624\n",
      "Epoch 731/1000, Loss: 0.39359837770462036\n",
      "Epoch 732/1000, Loss: 0.3078509569168091\n",
      "Epoch 733/1000, Loss: 0.41307127475738525\n",
      "Epoch 734/1000, Loss: 0.3777567148208618\n",
      "Epoch 735/1000, Loss: 0.4480355381965637\n",
      "Epoch 736/1000, Loss: 0.37742215394973755\n",
      "Epoch 737/1000, Loss: 0.3419538736343384\n",
      "Epoch 738/1000, Loss: 0.3770766854286194\n",
      "Epoch 739/1000, Loss: 0.34153008460998535\n",
      "Epoch 740/1000, Loss: 0.4259873032569885\n",
      "Epoch 741/1000, Loss: 0.4120124280452728\n",
      "Epoch 742/1000, Loss: 0.3763993978500366\n",
      "Epoch 743/1000, Loss: 0.34069621562957764\n",
      "Epoch 744/1000, Loss: 0.4116021990776062\n",
      "Epoch 745/1000, Loss: 0.3835059404373169\n",
      "Epoch 746/1000, Loss: 0.37569302320480347\n",
      "Epoch 747/1000, Loss: 0.3874649405479431\n",
      "Epoch 748/1000, Loss: 0.3753550946712494\n",
      "Epoch 749/1000, Loss: 0.3394273519515991\n",
      "Epoch 750/1000, Loss: 0.37499991059303284\n",
      "Epoch 751/1000, Loss: 0.4106506109237671\n",
      "Epoch 752/1000, Loss: 0.3746373653411865\n",
      "Epoch 753/1000, Loss: 0.4462893307209015\n",
      "Epoch 754/1000, Loss: 0.3858107328414917\n",
      "Epoch 755/1000, Loss: 0.3493655323982239\n",
      "Epoch 756/1000, Loss: 0.37393832206726074\n",
      "Epoch 757/1000, Loss: 0.37376129627227783\n",
      "Epoch 758/1000, Loss: 0.37358230352401733\n",
      "Epoch 759/1000, Loss: 0.45388275384902954\n",
      "Epoch 760/1000, Loss: 0.4094216227531433\n",
      "Epoch 761/1000, Loss: 0.3368263840675354\n",
      "Epoch 762/1000, Loss: 0.4091572165489197\n",
      "Epoch 763/1000, Loss: 0.3364177644252777\n",
      "Epoch 764/1000, Loss: 0.40890341997146606\n",
      "Epoch 765/1000, Loss: 0.37239405512809753\n",
      "Epoch 766/1000, Loss: 0.4086472988128662\n",
      "Epoch 767/1000, Loss: 0.3801792860031128\n",
      "Epoch 768/1000, Loss: 0.3719077706336975\n",
      "Epoch 769/1000, Loss: 0.4528449475765228\n",
      "Epoch 770/1000, Loss: 0.2619154453277588\n",
      "Epoch 771/1000, Loss: 0.334840327501297\n",
      "Epoch 772/1000, Loss: 0.4079117178916931\n",
      "Epoch 773/1000, Loss: 0.37111079692840576\n",
      "Epoch 774/1000, Loss: 0.41540253162384033\n",
      "Epoch 775/1000, Loss: 0.4442874789237976\n",
      "Epoch 776/1000, Loss: 0.4182915687561035\n",
      "Epoch 777/1000, Loss: 0.29682761430740356\n",
      "Epoch 778/1000, Loss: 0.3779548704624176\n",
      "Epoch 779/1000, Loss: 0.4070594310760498\n",
      "Epoch 780/1000, Loss: 0.41455715894699097\n",
      "Epoch 781/1000, Loss: 0.3698585033416748\n",
      "Epoch 782/1000, Loss: 0.369709312915802\n",
      "Epoch 783/1000, Loss: 0.3695610761642456\n",
      "Epoch 784/1000, Loss: 0.4064858555793762\n",
      "Epoch 785/1000, Loss: 0.44347941875457764\n",
      "Epoch 786/1000, Loss: 0.3319797217845917\n",
      "Epoch 787/1000, Loss: 0.3317932188510895\n",
      "Epoch 788/1000, Loss: 0.4060247242450714\n",
      "Epoch 789/1000, Loss: 0.44315576553344727\n",
      "Epoch 790/1000, Loss: 0.3684973418712616\n",
      "Epoch 791/1000, Loss: 0.48728007078170776\n",
      "Epoch 792/1000, Loss: 0.300351619720459\n",
      "Epoch 793/1000, Loss: 0.3748258948326111\n",
      "Epoch 794/1000, Loss: 0.3745613694190979\n",
      "Epoch 795/1000, Loss: 0.4492514729499817\n",
      "Epoch 796/1000, Loss: 0.330025851726532\n",
      "Epoch 797/1000, Loss: 0.41133829951286316\n",
      "Epoch 798/1000, Loss: 0.40481579303741455\n",
      "Epoch 799/1000, Loss: 0.36706775426864624\n",
      "Epoch 800/1000, Loss: 0.36690813302993774\n",
      "Epoch 801/1000, Loss: 0.329048216342926\n",
      "Epoch 802/1000, Loss: 0.3666018545627594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 803/1000, Loss: 0.32867395877838135\n",
      "Epoch 804/1000, Loss: 0.2906751036643982\n",
      "Epoch 805/1000, Loss: 0.4040016829967499\n",
      "Epoch 806/1000, Loss: 0.4099205732345581\n",
      "Epoch 807/1000, Loss: 0.2900124788284302\n",
      "Epoch 808/1000, Loss: 0.47958219051361084\n",
      "Epoch 809/1000, Loss: 0.29550808668136597\n",
      "Epoch 810/1000, Loss: 0.40933847427368164\n",
      "Epoch 811/1000, Loss: 0.3652583956718445\n",
      "Epoch 812/1000, Loss: 0.4413169026374817\n",
      "Epoch 813/1000, Loss: 0.4412418305873871\n",
      "Epoch 814/1000, Loss: 0.44117221236228943\n",
      "Epoch 815/1000, Loss: 0.3646899461746216\n",
      "Epoch 816/1000, Loss: 0.32631927728652954\n",
      "Epoch 817/1000, Loss: 0.4792463779449463\n",
      "Epoch 818/1000, Loss: 0.40260016918182373\n",
      "Epoch 819/1000, Loss: 0.40250545740127563\n",
      "Epoch 820/1000, Loss: 0.364042729139328\n",
      "Epoch 821/1000, Loss: 0.33167046308517456\n",
      "Epoch 822/1000, Loss: 0.36377865076065063\n",
      "Epoch 823/1000, Loss: 0.3251727223396301\n",
      "Epoch 824/1000, Loss: 0.44050562381744385\n",
      "Epoch 825/1000, Loss: 0.41706758737564087\n",
      "Epoch 826/1000, Loss: 0.4403637945652008\n",
      "Epoch 827/1000, Loss: 0.3630754351615906\n",
      "Epoch 828/1000, Loss: 0.45419126749038696\n",
      "Epoch 829/1000, Loss: 0.3241346478462219\n",
      "Epoch 830/1000, Loss: 0.33677253127098083\n",
      "Epoch 831/1000, Loss: 0.2530892491340637\n",
      "Epoch 832/1000, Loss: 0.43996554613113403\n",
      "Epoch 833/1000, Loss: 0.32344889640808105\n",
      "Epoch 834/1000, Loss: 0.3621191084384918\n",
      "Epoch 835/1000, Loss: 0.361971914768219\n",
      "Epoch 836/1000, Loss: 0.40075039863586426\n",
      "Epoch 837/1000, Loss: 0.3227175772190094\n",
      "Epoch 838/1000, Loss: 0.4005374610424042\n",
      "Epoch 839/1000, Loss: 0.29037395119667053\n",
      "Epoch 840/1000, Loss: 0.4003261923789978\n",
      "Epoch 841/1000, Loss: 0.2828899323940277\n",
      "Epoch 842/1000, Loss: 0.3609623312950134\n",
      "Epoch 843/1000, Loss: 0.4461561441421509\n",
      "Epoch 844/1000, Loss: 0.40681856870651245\n",
      "Epoch 845/1000, Loss: 0.36740174889564514\n",
      "Epoch 846/1000, Loss: 0.4065001606941223\n",
      "Epoch 847/1000, Loss: 0.3995835781097412\n",
      "Epoch 848/1000, Loss: 0.41125985980033875\n",
      "Epoch 849/1000, Loss: 0.4387860894203186\n",
      "Epoch 850/1000, Loss: 0.39928871393203735\n",
      "Epoch 851/1000, Loss: 0.3991926312446594\n",
      "Epoch 852/1000, Loss: 0.4385983943939209\n",
      "Epoch 853/1000, Loss: 0.4385373592376709\n",
      "Epoch 854/1000, Loss: 0.3989090919494629\n",
      "Epoch 855/1000, Loss: 0.398813396692276\n",
      "Epoch 856/1000, Loss: 0.44508183002471924\n",
      "Epoch 857/1000, Loss: 0.47796154022216797\n",
      "Epoch 858/1000, Loss: 0.43823081254959106\n",
      "Epoch 859/1000, Loss: 0.3984377086162567\n",
      "Epoch 860/1000, Loss: 0.31881773471832275\n",
      "Epoch 861/1000, Loss: 0.31866347789764404\n",
      "Epoch 862/1000, Loss: 0.31850430369377136\n",
      "Epoch 863/1000, Loss: 0.43793928623199463\n",
      "Epoch 864/1000, Loss: 0.3249036371707916\n",
      "Epoch 865/1000, Loss: 0.36913204193115234\n",
      "Epoch 866/1000, Loss: 0.31783902645111084\n",
      "Epoch 867/1000, Loss: 0.32452449202537537\n",
      "Epoch 868/1000, Loss: 0.5577643513679504\n",
      "Epoch 869/1000, Loss: 0.35740888118743896\n",
      "Epoch 870/1000, Loss: 0.3572821617126465\n",
      "Epoch 871/1000, Loss: 0.40425992012023926\n",
      "Epoch 872/1000, Loss: 0.3972177505493164\n",
      "Epoch 873/1000, Loss: 0.4775533378124237\n",
      "Epoch 874/1000, Loss: 0.2763177752494812\n",
      "Epoch 875/1000, Loss: 0.3566782772541046\n",
      "Epoch 876/1000, Loss: 0.3565504550933838\n",
      "Epoch 877/1000, Loss: 0.28265029191970825\n",
      "Epoch 878/1000, Loss: 0.4470841884613037\n",
      "Epoch 879/1000, Loss: 0.31576356291770935\n",
      "Epoch 880/1000, Loss: 0.4467487931251526\n",
      "Epoch 881/1000, Loss: 0.3559347689151764\n",
      "Epoch 882/1000, Loss: 0.31530407071113586\n",
      "Epoch 883/1000, Loss: 0.36275047063827515\n",
      "Epoch 884/1000, Loss: 0.23384088277816772\n",
      "Epoch 885/1000, Loss: 0.3960574269294739\n",
      "Epoch 886/1000, Loss: 0.43661266565322876\n",
      "Epoch 887/1000, Loss: 0.31452232599258423\n",
      "Epoch 888/1000, Loss: 0.43650639057159424\n",
      "Epoch 889/1000, Loss: 0.2806498408317566\n",
      "Epoch 890/1000, Loss: 0.395621120929718\n",
      "Epoch 891/1000, Loss: 0.31391674280166626\n",
      "Epoch 892/1000, Loss: 0.3137657344341278\n",
      "Epoch 893/1000, Loss: 0.39536407589912415\n",
      "Epoch 894/1000, Loss: 0.32056862115859985\n",
      "Epoch 895/1000, Loss: 0.3542448580265045\n",
      "Epoch 896/1000, Loss: 0.3950995206832886\n",
      "Epoch 897/1000, Loss: 0.4360243082046509\n",
      "Epoch 898/1000, Loss: 0.47701776027679443\n",
      "Epoch 899/1000, Loss: 0.401927649974823\n",
      "Epoch 900/1000, Loss: 0.31962722539901733\n",
      "Epoch 901/1000, Loss: 0.31241631507873535\n",
      "Epoch 902/1000, Loss: 0.3534450829029083\n",
      "Epoch 903/1000, Loss: 0.27094054222106934\n",
      "Epoch 904/1000, Loss: 0.35322147607803345\n",
      "Epoch 905/1000, Loss: 0.35310137271881104\n",
      "Epoch 906/1000, Loss: 0.35298290848731995\n",
      "Epoch 907/1000, Loss: 0.45541054010391235\n",
      "Epoch 908/1000, Loss: 0.36008286476135254\n",
      "Epoch 909/1000, Loss: 0.3190028667449951\n",
      "Epoch 910/1000, Loss: 0.4015484154224396\n",
      "Epoch 911/1000, Loss: 0.27683937549591064\n",
      "Epoch 912/1000, Loss: 0.3593282699584961\n",
      "Epoch 913/1000, Loss: 0.26924043893814087\n",
      "Epoch 914/1000, Loss: 0.48329001665115356\n",
      "Epoch 915/1000, Loss: 0.35822543501853943\n",
      "Epoch 916/1000, Loss: 0.40120503306388855\n",
      "Epoch 917/1000, Loss: 0.35181811451911926\n",
      "Epoch 918/1000, Loss: 0.49791085720062256\n",
      "Epoch 919/1000, Loss: 0.4391361474990845\n",
      "Epoch 920/1000, Loss: 0.35485219955444336\n",
      "Epoch 921/1000, Loss: 0.5053948163986206\n",
      "Epoch 922/1000, Loss: 0.3516961336135864\n",
      "Epoch 923/1000, Loss: 0.360854834318161\n",
      "Epoch 924/1000, Loss: 0.34868013858795166\n",
      "Epoch 925/1000, Loss: 0.39415764808654785\n",
      "Epoch 926/1000, Loss: 0.300426721572876\n",
      "Epoch 927/1000, Loss: 0.32519441843032837\n",
      "Epoch 928/1000, Loss: 0.3296370506286621\n",
      "Epoch 929/1000, Loss: 0.29146742820739746\n",
      "Epoch 930/1000, Loss: 0.4531534016132355\n",
      "Epoch 931/1000, Loss: 0.36818093061447144\n",
      "Epoch 932/1000, Loss: 0.41971397399902344\n",
      "Epoch 933/1000, Loss: 0.32775598764419556\n",
      "Epoch 934/1000, Loss: 0.3093125522136688\n",
      "Epoch 935/1000, Loss: 0.39355891942977905\n",
      "Epoch 936/1000, Loss: 0.2865827977657318\n",
      "Epoch 937/1000, Loss: 0.37299594283103943\n",
      "Epoch 938/1000, Loss: 0.30858713388442993\n",
      "Epoch 939/1000, Loss: 0.3084055185317993\n",
      "Epoch 940/1000, Loss: 0.282534122467041\n",
      "Epoch 941/1000, Loss: 0.5037868022918701\n",
      "Epoch 942/1000, Loss: 0.37751489877700806\n",
      "Epoch 943/1000, Loss: 0.22270816564559937\n",
      "Epoch 944/1000, Loss: 0.4194660186767578\n",
      "Epoch 945/1000, Loss: 0.3333044648170471\n",
      "Epoch 946/1000, Loss: 0.3669593930244446\n",
      "Epoch 947/1000, Loss: 0.3679344058036804\n",
      "Epoch 948/1000, Loss: 0.32930782437324524\n",
      "Epoch 949/1000, Loss: 0.4138392508029938\n",
      "Epoch 950/1000, Loss: 0.412411093711853\n",
      "Epoch 951/1000, Loss: 0.3917335569858551\n",
      "Epoch 952/1000, Loss: 0.2906114161014557\n",
      "Epoch 953/1000, Loss: 0.36267712712287903\n",
      "Epoch 954/1000, Loss: 0.26292669773101807\n",
      "Epoch 955/1000, Loss: 0.4285386800765991\n",
      "Epoch 956/1000, Loss: 0.3912197947502136\n",
      "Epoch 957/1000, Loss: 0.5143225193023682\n",
      "Epoch 958/1000, Loss: 0.3593543767929077\n",
      "Epoch 959/1000, Loss: 0.29637575149536133\n",
      "Epoch 960/1000, Loss: 0.43389931321144104\n",
      "Epoch 961/1000, Loss: 0.40313857793807983\n",
      "Epoch 962/1000, Loss: 0.23028221726417542\n",
      "Epoch 963/1000, Loss: 0.4452502429485321\n",
      "Epoch 964/1000, Loss: 0.3826413154602051\n",
      "Epoch 965/1000, Loss: 0.347237765789032\n",
      "Epoch 966/1000, Loss: 0.30390065908432007\n",
      "Epoch 967/1000, Loss: 0.48978185653686523\n",
      "Epoch 968/1000, Loss: 0.31741225719451904\n",
      "Epoch 969/1000, Loss: 0.42024973034858704\n",
      "Epoch 970/1000, Loss: 0.4481315016746521\n",
      "Epoch 971/1000, Loss: 0.36188703775405884\n",
      "Epoch 972/1000, Loss: 0.44899865984916687\n",
      "Epoch 973/1000, Loss: 0.31884339451789856\n",
      "Epoch 974/1000, Loss: 0.33051443099975586\n",
      "Epoch 975/1000, Loss: 0.274804025888443\n",
      "Epoch 976/1000, Loss: 0.3025684356689453\n",
      "Epoch 977/1000, Loss: 0.44820305705070496\n",
      "Epoch 978/1000, Loss: 0.27356424927711487\n",
      "Epoch 979/1000, Loss: 0.4476211667060852\n",
      "Epoch 980/1000, Loss: 0.3456552028656006\n",
      "Epoch 981/1000, Loss: 0.4207479655742645\n",
      "Epoch 982/1000, Loss: 0.3587307631969452\n",
      "Epoch 983/1000, Loss: 0.38907715678215027\n",
      "Epoch 984/1000, Loss: 0.3137241005897522\n",
      "Epoch 985/1000, Loss: 0.2575264871120453\n",
      "Epoch 986/1000, Loss: 0.35542476177215576\n",
      "Epoch 987/1000, Loss: 0.38503777980804443\n",
      "Epoch 988/1000, Loss: 0.4419408142566681\n",
      "Epoch 989/1000, Loss: 0.48581671714782715\n",
      "Epoch 990/1000, Loss: 0.38856133818626404\n",
      "Epoch 991/1000, Loss: 0.48591846227645874\n",
      "Epoch 992/1000, Loss: 0.43244367837905884\n",
      "Epoch 993/1000, Loss: 0.3538723587989807\n",
      "Epoch 994/1000, Loss: 0.3975255489349365\n",
      "Epoch 995/1000, Loss: 0.30009081959724426\n",
      "Epoch 996/1000, Loss: 0.30794215202331543\n",
      "Epoch 997/1000, Loss: 0.35128238797187805\n",
      "Epoch 998/1000, Loss: 0.3950044512748718\n",
      "Epoch 999/1000, Loss: 0.35099613666534424\n",
      "Epoch 1000/1000, Loss: 0.39545953273773193\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "loss_history =torch.tensor([])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_sequences, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1) Calculate the sequence lengths for the current batch\n",
    "        lengths = torch.sum(batch_sequences.sum(dim=2) != 0, dim=1)\n",
    "\n",
    "        batch_sequences = batch_sequences.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # 2) Forward pass\n",
    "        output = model(batch_sequences, lengths)\n",
    "        \n",
    "        # this is possible, but I did instead output squeeze to match dimension\n",
    "        # batch_labels = torch.unsqueeze(batch_labels, 1)\n",
    "\n",
    "        # 3) Compute the loss       \n",
    "        loss = criterion(output.squeeze(), batch_labels.float())\n",
    "\n",
    "        # 4) Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5) Gradient Clipping\n",
    "        # 5-1) Gradient Norm Clipping\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        # 5-2) Gradient Value Clipping\n",
    "        # nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "       \n",
    "        optimizer.step()\n",
    "\n",
    "    loss_history = torch.cat([loss_history, torch.tensor([loss.item()]).float()], dim=0)\n",
    "    #scheduler.step()\n",
    "    # Print the loss for every epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "287126e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1,num_epochs+1), loss_history, label = \"train loss\", linewidth=3)\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"loss\", fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135cedc",
   "metadata": {},
   "source": [
    "* __packed padding__: ok\n",
    "    - resolving unreasonable overshooting\n",
    "    - resolving the issue of zero padding and not knowing the original length\n",
    "* __batch normalization__\n",
    "    - so far overshooting\n",
    "* __learning rate__\n",
    "    - a bit large (0.001 -> 0.005) works better\n",
    "* __layer and hidden size__: ok\n",
    "    - not too much large, but larger one works well\n",
    "* last batch dropout\n",
    "    - works okay\n",
    "* __gradient clipping__\n",
    "* __gradient scheduler for decay__\n",
    "\n",
    "Not yet applied\n",
    "* dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30668dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test data: 90.0 %\n",
      "Precision: 0.9\n",
      "Recall: 0.9\n",
      "F1-score: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch_sequences, batch_targets in test_loader:\n",
    "        # Data pre-processing\n",
    "        lengths = torch.sum(batch_sequences.sum(dim=2) != 0, dim=1)\n",
    "        batch_sequences = batch_sequences.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch_sequences, lengths)\n",
    "        total += batch_targets.size(0)      \n",
    "\n",
    "        ### prediction\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "        #_, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        correct_tensor = pred.eq(batch_targets.float().view_as(pred))\n",
    "        current_correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        correct += np.sum(current_correct)\n",
    "        \n",
    "        y_true.extend(batch_targets.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "\n",
    "    print('Test Accuracy of the model on the test data: {} %'.format(100 * correct / total))\n",
    "\n",
    "    # Calculate other evaluation metrics (precision, recall, F1-score)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "90dc50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-use-lstm-for-a-time-series-classification-task/130559/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
