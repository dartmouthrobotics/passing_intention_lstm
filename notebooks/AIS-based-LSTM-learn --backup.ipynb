{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f12595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# when CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment\n",
    "# https://stackoverflow.com/a/73992912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756d8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# python modules\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# custom module\n",
    "from aux_code.learning_preprocess import convert_from_NED_to_Robotic, get_trajectory_before_pass, check_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f99193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab6f09",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0cf124",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"aisdk_20180101\"\n",
    "howmany=200\n",
    "using_file_indices = np.arange(0,howmany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b4d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire_pass = pd.DataFrame()\n",
    "\n",
    "# https://www.w3resource.com/pandas/dataframe/dataframe-to_pickle.php\n",
    "for using_file_idx in using_file_indices:\n",
    "    unpickled_df = pd.read_pickle(\"./dk_csv_20180101/binary/aisdk_20180101_{}.pkl\".format(str(using_file_idx)))\n",
    "    unpickled_df\n",
    "    df_entire_pass = pd.concat([df_entire_pass, unpickled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ad7a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:30</th>\n",
       "      <td>4.214136</td>\n",
       "      <td>2.391927</td>\n",
       "      <td>-0.009386</td>\n",
       "      <td>-0.006086</td>\n",
       "      <td>4.845643</td>\n",
       "      <td>0.516252</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>145.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:45</th>\n",
       "      <td>4.086469</td>\n",
       "      <td>2.312937</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>4.695626</td>\n",
       "      <td>0.515043</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>145.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:00</th>\n",
       "      <td>3.972708</td>\n",
       "      <td>2.235017</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>4.558257</td>\n",
       "      <td>0.512460</td>\n",
       "      <td>-0.002584</td>\n",
       "      <td>147.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:15</th>\n",
       "      <td>3.857949</td>\n",
       "      <td>2.154808</td>\n",
       "      <td>-0.007651</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>4.418933</td>\n",
       "      <td>0.509374</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>150.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:30</th>\n",
       "      <td>3.730337</td>\n",
       "      <td>2.078144</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>4.270140</td>\n",
       "      <td>0.508272</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>150.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:00</th>\n",
       "      <td>-0.388895</td>\n",
       "      <td>0.516592</td>\n",
       "      <td>-0.002982</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.646611</td>\n",
       "      <td>2.216093</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>235.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:15</th>\n",
       "      <td>-0.433163</td>\n",
       "      <td>0.564382</td>\n",
       "      <td>-0.002951</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.711447</td>\n",
       "      <td>2.225403</td>\n",
       "      <td>0.009310</td>\n",
       "      <td>235.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:30</th>\n",
       "      <td>-0.475801</td>\n",
       "      <td>0.610369</td>\n",
       "      <td>-0.002843</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.773910</td>\n",
       "      <td>2.232930</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:45</th>\n",
       "      <td>-0.520051</td>\n",
       "      <td>0.660504</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.840666</td>\n",
       "      <td>2.237780</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:38:00</th>\n",
       "      <td>-0.580154</td>\n",
       "      <td>0.725169</td>\n",
       "      <td>-0.004007</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.928681</td>\n",
       "      <td>2.245553</td>\n",
       "      <td>0.007774</td>\n",
       "      <td>231.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52343 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            x         y       v_x       v_y  rel_dist  \\\n",
       "Timestamp                                                               \n",
       "2018-01-01 23:34:30  4.214136  2.391927 -0.009386 -0.006086  4.845643   \n",
       "2018-01-01 23:34:45  4.086469  2.312937 -0.008511 -0.005266  4.695626   \n",
       "2018-01-01 23:35:00  3.972708  2.235017 -0.007584 -0.005195  4.558257   \n",
       "2018-01-01 23:35:15  3.857949  2.154808 -0.007651 -0.005347  4.418933   \n",
       "2018-01-01 23:35:30  3.730337  2.078144 -0.008507 -0.005111  4.270140   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2018-01-01 11:37:00 -0.388895  0.516592 -0.002982  0.003150  0.646611   \n",
       "2018-01-01 11:37:15 -0.433163  0.564382 -0.002951  0.003186  0.711447   \n",
       "2018-01-01 11:37:30 -0.475801  0.610369 -0.002843  0.003066  0.773910   \n",
       "2018-01-01 11:37:45 -0.520051  0.660504 -0.002950  0.003342  0.840666   \n",
       "2018-01-01 11:38:00 -0.580154  0.725169 -0.004007  0.004311  0.928681   \n",
       "\n",
       "                     rel_bearing  rel_bearing_diff  Heading  valid  obj_index  \\\n",
       "Timestamp                                                                       \n",
       "2018-01-01 23:34:30     0.516252         -0.001975    145.0   True          0   \n",
       "2018-01-01 23:34:45     0.515043         -0.001209    145.5   True          0   \n",
       "2018-01-01 23:35:00     0.512460         -0.002584    147.0   True          0   \n",
       "2018-01-01 23:35:15     0.509374         -0.003086    150.5   True          0   \n",
       "2018-01-01 23:35:30     0.508272         -0.001102    150.0   True          0   \n",
       "...                          ...               ...      ...    ...        ...   \n",
       "2018-01-01 11:37:00     2.216093          0.012567    235.0   True        199   \n",
       "2018-01-01 11:37:15     2.225403          0.009310    235.0   True        199   \n",
       "2018-01-01 11:37:30     2.232930          0.007527    234.0   True        199   \n",
       "2018-01-01 11:37:45     2.237780          0.004850    233.0   True        199   \n",
       "2018-01-01 11:38:00     2.245553          0.007774    231.0   True        199   \n",
       "\n",
       "                    label  \n",
       "Timestamp                  \n",
       "2018-01-01 23:34:30     L  \n",
       "2018-01-01 23:34:45     L  \n",
       "2018-01-01 23:35:00     L  \n",
       "2018-01-01 23:35:15     L  \n",
       "2018-01-01 23:35:30     L  \n",
       "...                   ...  \n",
       "2018-01-01 11:37:00     L  \n",
       "2018-01-01 11:37:15     L  \n",
       "2018-01-01 11:37:30     L  \n",
       "2018-01-01 11:37:45     L  \n",
       "2018-01-01 11:38:00     L  \n",
       "\n",
       "[52343 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entire_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde3dd4",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e6d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire_pass['heading_converted']  = np.deg2rad(df_entire_pass['Heading'])\n",
    "# lambda function method: very fast! \n",
    "# https://stackoverflow.com/questions/71249186/applying-function-to-column-in-a-dataframe\n",
    "df_entire_pass['heading_converted'] = df_entire_pass['heading_converted'].apply(convert_from_NED_to_Robotic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8034fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "      <th>heading_converted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:30</th>\n",
       "      <td>4.214136</td>\n",
       "      <td>2.391927</td>\n",
       "      <td>-0.009386</td>\n",
       "      <td>-0.006086</td>\n",
       "      <td>4.845643</td>\n",
       "      <td>0.516252</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>145.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.959931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:34:45</th>\n",
       "      <td>4.086469</td>\n",
       "      <td>2.312937</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>4.695626</td>\n",
       "      <td>0.515043</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>145.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.968658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:00</th>\n",
       "      <td>3.972708</td>\n",
       "      <td>2.235017</td>\n",
       "      <td>-0.007584</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>4.558257</td>\n",
       "      <td>0.512460</td>\n",
       "      <td>-0.002584</td>\n",
       "      <td>147.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-0.994838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:15</th>\n",
       "      <td>3.857949</td>\n",
       "      <td>2.154808</td>\n",
       "      <td>-0.007651</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>4.418933</td>\n",
       "      <td>0.509374</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>150.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.055924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 23:35:30</th>\n",
       "      <td>3.730337</td>\n",
       "      <td>2.078144</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>4.270140</td>\n",
       "      <td>0.508272</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>150.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>-1.047198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:00</th>\n",
       "      <td>-0.388895</td>\n",
       "      <td>0.516592</td>\n",
       "      <td>-0.002982</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.646611</td>\n",
       "      <td>2.216093</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>235.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.530727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:15</th>\n",
       "      <td>-0.433163</td>\n",
       "      <td>0.564382</td>\n",
       "      <td>-0.002951</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.711447</td>\n",
       "      <td>2.225403</td>\n",
       "      <td>0.009310</td>\n",
       "      <td>235.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.530727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:30</th>\n",
       "      <td>-0.475801</td>\n",
       "      <td>0.610369</td>\n",
       "      <td>-0.002843</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.773910</td>\n",
       "      <td>2.232930</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.513274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:37:45</th>\n",
       "      <td>-0.520051</td>\n",
       "      <td>0.660504</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.840666</td>\n",
       "      <td>2.237780</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.495821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:38:00</th>\n",
       "      <td>-0.580154</td>\n",
       "      <td>0.725169</td>\n",
       "      <td>-0.004007</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.928681</td>\n",
       "      <td>2.245553</td>\n",
       "      <td>0.007774</td>\n",
       "      <td>231.0</td>\n",
       "      <td>True</td>\n",
       "      <td>199</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.460914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52343 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            x         y       v_x       v_y  rel_dist  \\\n",
       "Timestamp                                                               \n",
       "2018-01-01 23:34:30  4.214136  2.391927 -0.009386 -0.006086  4.845643   \n",
       "2018-01-01 23:34:45  4.086469  2.312937 -0.008511 -0.005266  4.695626   \n",
       "2018-01-01 23:35:00  3.972708  2.235017 -0.007584 -0.005195  4.558257   \n",
       "2018-01-01 23:35:15  3.857949  2.154808 -0.007651 -0.005347  4.418933   \n",
       "2018-01-01 23:35:30  3.730337  2.078144 -0.008507 -0.005111  4.270140   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2018-01-01 11:37:00 -0.388895  0.516592 -0.002982  0.003150  0.646611   \n",
       "2018-01-01 11:37:15 -0.433163  0.564382 -0.002951  0.003186  0.711447   \n",
       "2018-01-01 11:37:30 -0.475801  0.610369 -0.002843  0.003066  0.773910   \n",
       "2018-01-01 11:37:45 -0.520051  0.660504 -0.002950  0.003342  0.840666   \n",
       "2018-01-01 11:38:00 -0.580154  0.725169 -0.004007  0.004311  0.928681   \n",
       "\n",
       "                     rel_bearing  rel_bearing_diff  Heading  valid  obj_index  \\\n",
       "Timestamp                                                                       \n",
       "2018-01-01 23:34:30     0.516252         -0.001975    145.0   True          0   \n",
       "2018-01-01 23:34:45     0.515043         -0.001209    145.5   True          0   \n",
       "2018-01-01 23:35:00     0.512460         -0.002584    147.0   True          0   \n",
       "2018-01-01 23:35:15     0.509374         -0.003086    150.5   True          0   \n",
       "2018-01-01 23:35:30     0.508272         -0.001102    150.0   True          0   \n",
       "...                          ...               ...      ...    ...        ...   \n",
       "2018-01-01 11:37:00     2.216093          0.012567    235.0   True        199   \n",
       "2018-01-01 11:37:15     2.225403          0.009310    235.0   True        199   \n",
       "2018-01-01 11:37:30     2.232930          0.007527    234.0   True        199   \n",
       "2018-01-01 11:37:45     2.237780          0.004850    233.0   True        199   \n",
       "2018-01-01 11:38:00     2.245553          0.007774    231.0   True        199   \n",
       "\n",
       "                    label  heading_converted  \n",
       "Timestamp                                     \n",
       "2018-01-01 23:34:30     L          -0.959931  \n",
       "2018-01-01 23:34:45     L          -0.968658  \n",
       "2018-01-01 23:35:00     L          -0.994838  \n",
       "2018-01-01 23:35:15     L          -1.055924  \n",
       "2018-01-01 23:35:30     L          -1.047198  \n",
       "...                   ...                ...  \n",
       "2018-01-01 11:37:00     L          -2.530727  \n",
       "2018-01-01 11:37:15     L          -2.530727  \n",
       "2018-01-01 11:37:30     L          -2.513274  \n",
       "2018-01-01 11:37:45     L          -2.495821  \n",
       "2018-01-01 11:38:00     L          -2.460914  \n",
       "\n",
       "[52343 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entire_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d392c52",
   "metadata": {},
   "source": [
    "## Train, test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526595ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 160 \n",
      " test size: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 18, 170, 107,  98, 177, 182,   5, 146,  12, 152,  61, 125, 180,\n",
       "       154,  80,   7,  33, 130,  37,  74, 183, 145,  45, 159,  60, 123,\n",
       "       179, 185, 122,  44,  16,  55, 150, 111,  22, 189, 129,   4,  83,\n",
       "       106, 134,  66,  26, 113, 168,  63,   8,  75, 118, 143,  71, 124,\n",
       "       184,  97, 149,  24,  30, 160,  40,  56, 131,  96, 181,  19, 153,\n",
       "        92,  54, 163,  51,  86, 139,  90, 137, 101, 144,  89, 109,  14,\n",
       "        27, 141, 187,  46, 138, 195, 108,  62,   2,  59, 136, 197,  43,\n",
       "        10, 194,  73, 196, 178, 175, 126,  93, 112, 158, 191,  50,   0,\n",
       "        94, 110,  95,  64, 167,  41,  69,  49,  48,  85,  13, 161,  23,\n",
       "       186, 135,  20,  15,  78, 104,  52, 100,  76,   3, 116, 164, 198,\n",
       "         6,  68,  84, 121, 155, 171, 156,  91, 199,  11, 119, 102,  35,\n",
       "        57,  65,   1, 120, 162,  42, 105, 132, 173,  17,  38, 133,  53,\n",
       "       157, 128,  34,  28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/how-to-randomly-select-elements-of-an-array-with-numpy-in-python/\n",
    "\n",
    "np.random.seed(0) # right now for consistent data testing\n",
    "\n",
    "\n",
    "# for train, test data\n",
    "use_df = df_entire_pass.loc[(df_entire_pass.valid == True)] # to be used, valid df\n",
    "unique_id = use_df.obj_index.unique()\n",
    "\n",
    "# data split\n",
    "train_data_size = int(len(unique_id) * 0.8)\n",
    "test_data_size = len(unique_id) - train_data_size\n",
    "print(\"train size: {} \\n test size: {}\".format(train_data_size, test_data_size))\n",
    "\n",
    "# split obj indexes\n",
    "train_obj_id = np.random.choice(unique_id, size = train_data_size, replace=False)\n",
    "test_obj_id = np.setdiff1d(unique_id, train_obj_id)\n",
    "train_obj_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27158cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "      <th>heading_converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.132110</td>\n",
       "      <td>-2.747226</td>\n",
       "      <td>-0.005239</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>4.962014</td>\n",
       "      <td>-0.586743</td>\n",
       "      <td>-0.001738</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.426008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.047040</td>\n",
       "      <td>-2.700071</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>4.865071</td>\n",
       "      <td>-0.588352</td>\n",
       "      <td>-0.001610</td>\n",
       "      <td>228.833333</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.423099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.965174</td>\n",
       "      <td>-2.653981</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.003073</td>\n",
       "      <td>4.771396</td>\n",
       "      <td>-0.589839</td>\n",
       "      <td>-0.001487</td>\n",
       "      <td>228.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.420190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.859062</td>\n",
       "      <td>-2.608618</td>\n",
       "      <td>-0.007074</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>4.658030</td>\n",
       "      <td>-0.594417</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>228.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.417281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.772702</td>\n",
       "      <td>-2.562771</td>\n",
       "      <td>-0.005757</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>4.560820</td>\n",
       "      <td>-0.596693</td>\n",
       "      <td>-0.002276</td>\n",
       "      <td>228.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.414372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32650</th>\n",
       "      <td>0.284465</td>\n",
       "      <td>-0.223252</td>\n",
       "      <td>-0.003810</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.361609</td>\n",
       "      <td>-0.665412</td>\n",
       "      <td>0.032140</td>\n",
       "      <td>232.846154</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.493136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32651</th>\n",
       "      <td>0.227320</td>\n",
       "      <td>-0.160196</td>\n",
       "      <td>-0.003810</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.278096</td>\n",
       "      <td>-0.613884</td>\n",
       "      <td>0.051528</td>\n",
       "      <td>233.538462</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.505219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32652</th>\n",
       "      <td>0.170175</td>\n",
       "      <td>-0.097141</td>\n",
       "      <td>-0.003810</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.195949</td>\n",
       "      <td>-0.518692</td>\n",
       "      <td>0.095192</td>\n",
       "      <td>234.230769</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.517302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32653</th>\n",
       "      <td>0.113031</td>\n",
       "      <td>-0.034085</td>\n",
       "      <td>-0.003810</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.118058</td>\n",
       "      <td>-0.292885</td>\n",
       "      <td>0.225808</td>\n",
       "      <td>234.923077</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.529385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32654</th>\n",
       "      <td>0.048061</td>\n",
       "      <td>0.034145</td>\n",
       "      <td>-0.004331</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.058955</td>\n",
       "      <td>0.617704</td>\n",
       "      <td>0.910589</td>\n",
       "      <td>235.615385</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.541468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32655 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x         y       v_x       v_y  rel_dist  rel_bearing  \\\n",
       "0      4.132110 -2.747226 -0.005239  0.002781  4.962014    -0.586743   \n",
       "1      4.047040 -2.700071 -0.005671  0.003144  4.865071    -0.588352   \n",
       "2      3.965174 -2.653981 -0.005458  0.003073  4.771396    -0.589839   \n",
       "3      3.859062 -2.608618 -0.007074  0.003024  4.658030    -0.594417   \n",
       "4      3.772702 -2.562771 -0.005757  0.003056  4.560820    -0.596693   \n",
       "...         ...       ...       ...       ...       ...          ...   \n",
       "32650  0.284465 -0.223252 -0.003810  0.004204  0.361609    -0.665412   \n",
       "32651  0.227320 -0.160196 -0.003810  0.004204  0.278096    -0.613884   \n",
       "32652  0.170175 -0.097141 -0.003810  0.004204  0.195949    -0.518692   \n",
       "32653  0.113031 -0.034085 -0.003810  0.004204  0.118058    -0.292885   \n",
       "32654  0.048061  0.034145 -0.004331  0.004549  0.058955     0.617704   \n",
       "\n",
       "       rel_bearing_diff     Heading  valid  obj_index label  heading_converted  \n",
       "0             -0.001738  229.000000   True         18     L          -2.426008  \n",
       "1             -0.001610  228.833333   True         18     L          -2.423099  \n",
       "2             -0.001487  228.666667   True         18     L          -2.420190  \n",
       "3             -0.004578  228.500000   True         18     L          -2.417281  \n",
       "4             -0.002276  228.333333   True         18     L          -2.414372  \n",
       "...                 ...         ...    ...        ...   ...                ...  \n",
       "32650          0.032140  232.846154   True         28     L          -2.493136  \n",
       "32651          0.051528  233.538462   True         28     L          -2.505219  \n",
       "32652          0.095192  234.230769   True         28     L          -2.517302  \n",
       "32653          0.225808  234.923077   True         28     L          -2.529385  \n",
       "32654          0.910589  235.615385   True         28     L          -2.541468  \n",
       "\n",
       "[32655 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "df_cropped_train = get_trajectory_before_pass(df_entire_pass, train_obj_id)\n",
    "df_cropped_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e857f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(df_cropped_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11c1f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(df_cropped_train['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12df7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cropped_train = df_cropped_train[df_cropped_train['Heading_by_xy'].notna()]  # drop na for Heaing_by_xy\n",
    "# np.any(np.isnan(df_cropped_train['Heading_by_xy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336d5754",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>v_x</th>\n",
       "      <th>v_y</th>\n",
       "      <th>rel_dist</th>\n",
       "      <th>rel_bearing</th>\n",
       "      <th>rel_bearing_diff</th>\n",
       "      <th>Heading</th>\n",
       "      <th>valid</th>\n",
       "      <th>obj_index</th>\n",
       "      <th>label</th>\n",
       "      <th>heading_converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.531333</td>\n",
       "      <td>4.889139</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.007141</td>\n",
       "      <td>4.917926</td>\n",
       "      <td>1.462545</td>\n",
       "      <td>-0.002815</td>\n",
       "      <td>90.6250</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.010908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.531943</td>\n",
       "      <td>4.777563</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.007438</td>\n",
       "      <td>4.807085</td>\n",
       "      <td>1.459911</td>\n",
       "      <td>-0.002634</td>\n",
       "      <td>90.9375</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.531469</td>\n",
       "      <td>4.642245</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.009021</td>\n",
       "      <td>4.672568</td>\n",
       "      <td>1.456807</td>\n",
       "      <td>-0.003104</td>\n",
       "      <td>91.2500</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.021817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.534526</td>\n",
       "      <td>4.534370</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.007192</td>\n",
       "      <td>4.565767</td>\n",
       "      <td>1.453455</td>\n",
       "      <td>-0.003353</td>\n",
       "      <td>91.5625</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.027271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.537584</td>\n",
       "      <td>4.426495</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.007192</td>\n",
       "      <td>4.459020</td>\n",
       "      <td>1.449941</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>91.8750</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>-0.032725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12287</th>\n",
       "      <td>0.684961</td>\n",
       "      <td>0.278331</td>\n",
       "      <td>-0.010316</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.739351</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.129815</td>\n",
       "      <td>209.5000</td>\n",
       "      <td>True</td>\n",
       "      <td>193</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.085668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12288</th>\n",
       "      <td>0.501735</td>\n",
       "      <td>0.356633</td>\n",
       "      <td>-0.012215</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.615569</td>\n",
       "      <td>0.617937</td>\n",
       "      <td>0.231972</td>\n",
       "      <td>210.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>193</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12289</th>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.416676</td>\n",
       "      <td>-0.010073</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.544581</td>\n",
       "      <td>0.871248</td>\n",
       "      <td>0.253311</td>\n",
       "      <td>210.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>193</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.094395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12290</th>\n",
       "      <td>0.218809</td>\n",
       "      <td>0.477479</td>\n",
       "      <td>-0.008789</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.525227</td>\n",
       "      <td>1.141095</td>\n",
       "      <td>0.269848</td>\n",
       "      <td>209.5000</td>\n",
       "      <td>True</td>\n",
       "      <td>193</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.085668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12291</th>\n",
       "      <td>0.040190</td>\n",
       "      <td>0.547844</td>\n",
       "      <td>-0.011908</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.549316</td>\n",
       "      <td>1.497567</td>\n",
       "      <td>0.356472</td>\n",
       "      <td>209.5000</td>\n",
       "      <td>True</td>\n",
       "      <td>193</td>\n",
       "      <td>L</td>\n",
       "      <td>-2.085668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12292 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x         y       v_x       v_y  rel_dist  rel_bearing  \\\n",
       "0      0.531333  4.889139  0.000172 -0.007141  4.917926     1.462545   \n",
       "1      0.531943  4.777563  0.000041 -0.007438  4.807085     1.459911   \n",
       "2      0.531469  4.642245 -0.000032 -0.009021  4.672568     1.456807   \n",
       "3      0.534526  4.534370  0.000204 -0.007192  4.565767     1.453455   \n",
       "4      0.537584  4.426495  0.000204 -0.007192  4.459020     1.449941   \n",
       "...         ...       ...       ...       ...       ...          ...   \n",
       "12287  0.684961  0.278331 -0.010316  0.003894  0.739351     0.385965   \n",
       "12288  0.501735  0.356633 -0.012215  0.005220  0.615569     0.617937   \n",
       "12289  0.350640  0.416676 -0.010073  0.004003  0.544581     0.871248   \n",
       "12290  0.218809  0.477479 -0.008789  0.004053  0.525227     1.141095   \n",
       "12291  0.040190  0.547844 -0.011908  0.004691  0.549316     1.497567   \n",
       "\n",
       "       rel_bearing_diff   Heading  valid  obj_index label  heading_converted  \n",
       "0             -0.002815   90.6250   True          9     R          -0.010908  \n",
       "1             -0.002634   90.9375   True          9     R          -0.016362  \n",
       "2             -0.003104   91.2500   True          9     R          -0.021817  \n",
       "3             -0.003353   91.5625   True          9     R          -0.027271  \n",
       "4             -0.003513   91.8750   True          9     R          -0.032725  \n",
       "...                 ...       ...    ...        ...   ...                ...  \n",
       "12287          0.129815  209.5000   True        193     L          -2.085668  \n",
       "12288          0.231972  210.0000   True        193     L          -2.094395  \n",
       "12289          0.253311  210.0000   True        193     L          -2.094395  \n",
       "12290          0.269848  209.5000   True        193     L          -2.085668  \n",
       "12291          0.356472  209.5000   True        193     L          -2.085668  \n",
       "\n",
       "[12292 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "df_cropped_test = get_trajectory_before_pass(df_entire_pass, test_obj_id)\n",
    "df_cropped_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbfc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cropped_test = df_cropped_test[df_cropped_test['Heading_by_xy'].notna()] # drop na for Heaing_by_xy\n",
    "# np.any(np.isnan(df_cropped_test['Heading_by_xy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9785d11",
   "metadata": {},
   "source": [
    "## Train, test data onehot encoding and extract specific cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3e75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input, output dimension is same. good to go\n",
      "Input, output dimension is same. good to go\n"
     ]
    }
   ],
   "source": [
    "# columns to extract\n",
    "columns_for_x = ['x', 'y', 'rel_dist', 'rel_bearing', 'heading_converted']\n",
    "# columns_for_x = ['x', 'y', 'v_x', 'v_y', 'rel_dist', 'rel_bearing', 'heading_converted']\n",
    "# columns_for_x = ['r_value', 'atan']\n",
    "columns_for_y = ['label']\n",
    "binary_classification = True\n",
    "\n",
    "# one hot encoding for y target\n",
    "one_hot_lookup = torch.eye(2).tolist()\n",
    "\n",
    "# df to array\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "#### train dataset\n",
    "for obj_id, df_by_ID in df_cropped_train.groupby('obj_index'):\n",
    "    X_train.append(df_by_ID[columns_for_x].values.tolist())\n",
    "\n",
    "    if binary_classification:\n",
    "        # y_train.append(one_hot_lookup[0] if df_by_ID[columns_for_y].values[-1][0] == \"L\" else one_hot_lookup[1]) # last row, only char\n",
    "        y_train.append(0 if df_by_ID[columns_for_y].values[-1][0] == \"L\" else 1) # last row, only char\n",
    "    else:\n",
    "        # print(df_by_ID['bow_crossing'].values[-1] )\n",
    "        y_train.append(0 if df_by_ID[columns_for_y].values[-1][0] == \"L\" and df_by_ID['bow_crossing'].values[-1] == True\\\n",
    "                       else 1 if df_by_ID[columns_for_y].values[-1][0] == \"L\" and df_by_ID['bow_crossing'].values[-1] ==  False\\\n",
    "                       else 2 if df_by_ID[columns_for_y].values[-1][0] == \"R\" and df_by_ID['bow_crossing'].values[-1] == True \\\n",
    "                        else 3) # last row, only bool\n",
    "\n",
    "    #### double check train dataset\n",
    "check_dimension(X_train, y_train)\n",
    "\n",
    "#### test dataset\n",
    "for obj_id, df_by_ID in df_cropped_test.groupby('obj_index'):\n",
    "    X_test.append(df_by_ID[columns_for_x].values.tolist())\n",
    "    \n",
    "    if binary_classification:\n",
    "        # y_test.append(one_hot_lookup[0] if df_by_ID[columns_for_y].values[-1][0] == \"L\" else one_hot_lookup[1]) # last row, only char\n",
    "        y_test.append(0 if df_by_ID[columns_for_y].values[-1][0] == \"L\" else 1) # last row, only char\n",
    "    else:\n",
    "        y_test.append(0 if df_by_ID[columns_for_y].values[-1][0] == \"L\" and df_by_ID['bow_crossing'].values[-1] == True\\\n",
    "                       else 1 if df_by_ID[columns_for_y].values[-1][0] == \"L\" and df_by_ID['bow_crossing'].values[-1] ==  False\\\n",
    "                       else 2 if df_by_ID[columns_for_y].values[-1][0] == \"R\" and df_by_ID['bow_crossing'].values[-1] == True \\\n",
    "                        else 3) # last row, only bool\n",
    "\n",
    "# TODO 2023.08.23 \n",
    "# 1. choose one hot encoding and one_hot_lookup[0] or one_hot_lookup[1]\n",
    "# 2. output dimension as 2\n",
    "\n",
    "#### double check test dataset\n",
    "check_dimension(X_test, y_test)\n",
    "\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb68535",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032b8cd",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b59c5a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence_length = 10 # number of timestamps # padding\n",
    "input_size = 5 # number of columns, features\n",
    "batch_size = 1 # number of samples sent to the model at one time 32\n",
    "\n",
    "hidden_size = 64 # dimension of hidden state # 500\n",
    "num_layers = 6 # total layer\n",
    "num_classes = 1 # output class \n",
    "# binary: (L or R)  1 https://bhadreshpsavani.medium.com/tutorial-on-sentimental-analysis-using-pytorch-b1431306a2d7\n",
    "# multi: (LF, L, RF, R)\n",
    "num_epochs = 1\n",
    "learning_rate = 0.03 # 0.03 for binary\n",
    "clip=1 # gradient clipping\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38661355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "# sequences: List of variable-length sequences\n",
    "# targets: List of target labels\n",
    "\n",
    "def pad_sequential_data(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_data: (list) training or test X data, sequential, not equal length\n",
    "        y_data: (list) training or test y data\n",
    "\n",
    "    Returns:\n",
    "        - same length sequence for X_data\n",
    "\n",
    "    reference: https://chat.openai.com/c/235f65e4-3a26-4418-a88c-ecf521cc5d8d\n",
    "    \"\"\"\n",
    "    sequences = [torch.tensor(seq) for seq in X_data]\n",
    "    targets = torch.tensor(y_data)\n",
    "\n",
    "    #### Sort sequences by length in descending order\n",
    "    # sequences[i] indicatess i th object (variable length)\n",
    "    # sequence[i][0] (dimension: feature numbers)\n",
    "\n",
    "    # sorted_indices = sorted(range(len(sequences)), key=lambda i: len(sequences[i][0]), reverse=True)\n",
    "    sorted_indices = sorted(range(len(sequences)), key=lambda i: len(sequences[i]), reverse=True)\n",
    "\n",
    "    #### sorted as per length\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = targets[sorted_indices]\n",
    "\n",
    "    #### Pad the sequences to make them the same length (zero padding as default)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b37d305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3253, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Create a TensorDataset from the padded sequences and targets\n",
    "padded_sequences, targets = pad_sequential_data(X_train, y_train)\n",
    "train_dataset = TensorDataset(padded_sequences, targets)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "#### Getting train, validation data\n",
    "test_abs = int(len(train_dataset) * 0.8)\n",
    "train_subset, val_subset = random_split(train_dataset, [test_abs, len(train_dataset) - test_abs])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True) # https://stackoverflow.com/a/53286859\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#### Getting test data\n",
    "padded_sequences_test, targets_test = pad_sequential_data(X_test, y_test)\n",
    "test_dataset = TensorDataset(padded_sequences_test, targets_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # drop last didn't apply to test_loader to avoid null test_loader\n",
    "\n",
    "padded_sequences_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37db0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.5064, -2.1310,  4.9849, -0.4417, -2.8100],\n",
      "         [ 4.4811, -2.0693,  4.9358, -0.4326, -2.7925],\n",
      "         [ 4.4518, -2.0051,  4.8825, -0.4232, -2.7751],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "for tensor in test_loader:\n",
    "    print(tensor[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "916de49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f04f8de5160>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fcb1f",
   "metadata": {},
   "source": [
    "### Reference on model output channel, one hot encoding, BCEloss\n",
    "\n",
    "* https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/15\n",
    "* https://discuss.pytorch.org/t/same-output-when-using-multilabel-bce/104158/7\n",
    "\n",
    "* validation do we need to add activation at the last step for softmax?\n",
    "* conclusion\n",
    "    * `BCELoss` should use `sigmoid()`\n",
    "    * `BCEWithLogitsLoss` should not use `sigmoid()` as it internally contains\n",
    "    * a simple binary image with 1 channel would be enough.\n",
    "    * `BCEWithLogitsLoss` (and BCELoss) also do not take one-hot encoded targets. \n",
    "    * `CrossEntropyLoss` does not take one-hot-encoded targets, but\n",
    "    instead takes integer class labels, which, to reiterate, contain exactly\n",
    "    the same information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaf662",
   "metadata": {},
   "source": [
    "### LSTM 2D model with packed padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b918f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM2D, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "                    input_size, \n",
    "                    hidden_size, \n",
    "                    num_layers, \n",
    "                    batch_first=True,\n",
    "                    dropout = 0.3\n",
    "                    )\n",
    "        # self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_sequences = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        packed_out, _ = self.lstm(packed_sequences, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "\n",
    "        out = out[:, -1, :] # all batch, last time_series, all feature\n",
    "\n",
    "        # out = self.batch_norm(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # https://stackoverflow.com/questions/66456541/runtimeerror-cuda-error-device-side-assert-triggered-on-loss-function\n",
    "        if binary_classification:\n",
    "            # https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/15\n",
    "            # https://discuss.pytorch.org/t/same-output-when-using-multilabel-bce/104158/7\n",
    "            out = self.sigmoid(out) # ouput [0, 1]\n",
    "        \n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312683a5",
   "metadata": {},
   "source": [
    "### RNN define and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f5083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM2D(\n",
      "  (lstm): LSTM(5, 64, num_layers=6, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM2D(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "print(model)\n",
    "\n",
    "if binary_classification:\n",
    "    criterion = nn.BCELoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out 4 here tensor([[0.4882],\n",
      "        [0.4852],\n",
      "        [0.4875],\n",
      "        [0.4876],\n",
      "        [0.4871],\n",
      "        [0.4868],\n",
      "        [0.4876],\n",
      "        [0.4853],\n",
      "        [0.4869],\n",
      "        [0.4858],\n",
      "        [0.4878],\n",
      "        [0.4873],\n",
      "        [0.4872],\n",
      "        [0.4861],\n",
      "        [0.4865],\n",
      "        [0.4876],\n",
      "        [0.4847],\n",
      "        [0.4879],\n",
      "        [0.4865],\n",
      "        [0.4857],\n",
      "        [0.4850],\n",
      "        [0.4848],\n",
      "        [0.4855],\n",
      "        [0.4869],\n",
      "        [0.4880],\n",
      "        [0.4858],\n",
      "        [0.4870],\n",
      "        [0.4876],\n",
      "        [0.4867],\n",
      "        [0.4865],\n",
      "        [0.4881],\n",
      "        [0.4849]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "out 4 here tensor([[0.0696],\n",
      "        [0.0688],\n",
      "        [0.0676],\n",
      "        [0.0678],\n",
      "        [0.0702],\n",
      "        [0.0640],\n",
      "        [0.0655],\n",
      "        [0.0699],\n",
      "        [0.0681],\n",
      "        [0.0672],\n",
      "        [0.0732],\n",
      "        [0.0686],\n",
      "        [0.0674],\n",
      "        [0.0679],\n",
      "        [0.0657],\n",
      "        [0.0672],\n",
      "        [0.0700],\n",
      "        [0.0687],\n",
      "        [0.0710],\n",
      "        [0.0686],\n",
      "        [0.0659],\n",
      "        [0.0712],\n",
      "        [0.0714],\n",
      "        [0.0669],\n",
      "        [0.0725],\n",
      "        [0.0664],\n",
      "        [0.0674],\n",
      "        [0.0739],\n",
      "        [0.0696],\n",
      "        [0.0730],\n",
      "        [0.0702],\n",
      "        [0.0684]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "out 4 here tensor([[0.2409],\n",
      "        [0.2459],\n",
      "        [0.2374],\n",
      "        [0.2649],\n",
      "        [0.2188],\n",
      "        [0.2499],\n",
      "        [0.2312],\n",
      "        [0.2390],\n",
      "        [0.2525],\n",
      "        [0.2488],\n",
      "        [0.2482],\n",
      "        [0.2317],\n",
      "        [0.2416],\n",
      "        [0.2667],\n",
      "        [0.2414],\n",
      "        [0.2352],\n",
      "        [0.2326],\n",
      "        [0.2271],\n",
      "        [0.2632],\n",
      "        [0.2465],\n",
      "        [0.2470],\n",
      "        [0.2420],\n",
      "        [0.2401],\n",
      "        [0.2370],\n",
      "        [0.2243],\n",
      "        [0.2209],\n",
      "        [0.2465],\n",
      "        [0.2604],\n",
      "        [0.2517],\n",
      "        [0.2473],\n",
      "        [0.2296],\n",
      "        [0.2523]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "out 4 here tensor([[0.0548],\n",
      "        [0.0580],\n",
      "        [0.0633],\n",
      "        [0.0631],\n",
      "        [0.0563],\n",
      "        [0.0607],\n",
      "        [0.0579],\n",
      "        [0.0593],\n",
      "        [0.0532],\n",
      "        [0.0559],\n",
      "        [0.0585],\n",
      "        [0.0630],\n",
      "        [0.0544],\n",
      "        [0.0583],\n",
      "        [0.0623],\n",
      "        [0.0573],\n",
      "        [0.0568],\n",
      "        [0.0595],\n",
      "        [0.0550],\n",
      "        [0.0622],\n",
      "        [0.0541],\n",
      "        [0.0593],\n",
      "        [0.0603],\n",
      "        [0.0583],\n",
      "        [0.0589],\n",
      "        [0.0561],\n",
      "        [0.0575],\n",
      "        [0.0585],\n",
      "        [0.0692],\n",
      "        [0.0563],\n",
      "        [0.0561],\n",
      "        [0.0689]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Epoch 1/1, Training Loss: 0.40575551986694336\n",
      "out 4 here tensor([[0.0650],\n",
      "        [0.0704],\n",
      "        [0.0663],\n",
      "        [0.0647],\n",
      "        [0.0630],\n",
      "        [0.0610],\n",
      "        [0.0603],\n",
      "        [0.0627],\n",
      "        [0.0613],\n",
      "        [0.0685],\n",
      "        [0.0636],\n",
      "        [0.0649],\n",
      "        [0.0682],\n",
      "        [0.0667],\n",
      "        [0.0669],\n",
      "        [0.0613],\n",
      "        [0.0665],\n",
      "        [0.0631],\n",
      "        [0.0627],\n",
      "        [0.0698],\n",
      "        [0.0670],\n",
      "        [0.0742],\n",
      "        [0.0686],\n",
      "        [0.0649],\n",
      "        [0.0642],\n",
      "        [0.0697],\n",
      "        [0.0698],\n",
      "        [0.0636],\n",
      "        [0.0644],\n",
      "        [0.0644],\n",
      "        [0.0627],\n",
      "        [0.0709]], device='cuda:0')\n",
      "Epoch 1/1, Validation Loss: 0.23709014058113098\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "if binary_classification:\n",
    "\n",
    "    model.train()\n",
    "    loss_history =torch.tensor([])\n",
    "    val_loss_history =torch.tensor([])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ##################################################################\n",
    "        ##### training \n",
    "        ##################################################################\n",
    "        for batch_sequences, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1) Calculate the sequence lengths for the current batch\n",
    "            lengths = torch.sum(batch_sequences.sum(dim=2) != 0, dim=1)\n",
    "\n",
    "            batch_sequences = batch_sequences.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # 2) Forward pass\n",
    "            output = model(batch_sequences, lengths)\n",
    "            \n",
    "            # this is possible, but I did instead output squeeze to match dimension\n",
    "            # batch_labels = torch.unsqueeze(batch_labels, 1)\n",
    "\n",
    "            # 3) Compute the loss       \n",
    "            loss = criterion(output.squeeze(), batch_labels.float())\n",
    "\n",
    "            # 4) Backward pass and optimization\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5) Gradient Clipping\n",
    "            # 5-1) Gradient Norm Clipping\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            # 5-2) Gradient Value Clipping\n",
    "            # nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "\n",
    "        loss_history = torch.cat([loss_history, torch.tensor([loss.item()]).float()], dim=0)\n",
    "\n",
    "        # Print the loss for every epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}\")\n",
    "\n",
    "        ##################################################################\n",
    "        ##### Validation\n",
    "        ##################################################################\n",
    "        val_steps = 0\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            with torch.no_grad():\n",
    "                batch_sequences_val, batch_labels_val = data\n",
    "\n",
    "                # 1) Calculate the sequence lengths for the current batch\n",
    "                lengths_val = torch.sum(batch_sequences_val.sum(dim=2) != 0, dim=1)\n",
    "                batch_sequences_val = batch_sequences_val.to(device)\n",
    "                batch_labels_val = batch_labels_val.to(device)\n",
    "\n",
    "                # 2) Forward pass\n",
    "                output_val = model(batch_sequences_val, lengths_val)\n",
    "                total_val += batch_labels_val.size(0)      \n",
    "\n",
    "                ### prediction\n",
    "                pred_val = torch.round(output_val.squeeze())  # rounds to the nearest integer\n",
    "                #_, predicted = torch.max(output_val, 1)\n",
    "                \n",
    "                correct_tensor_val = pred_val.eq(batch_labels_val.float().view_as(pred_val))\n",
    "                current_correct_val = np.squeeze(correct_tensor_val.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor_val.cpu().numpy())\n",
    "                correct_val += np.sum(current_correct_val)\n",
    "\n",
    "                loss_val = criterion(output_val.squeeze(), batch_labels_val.float())\n",
    "                val_steps += 1\n",
    "\n",
    "        val_loss_history = torch.cat([val_loss_history, torch.tensor([loss_val.item()]).float()], dim=0)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {loss_val.item()}\")\n",
    "\n",
    "        \n",
    "        ##################################################################\n",
    "        ##### scheduler\n",
    "        ##################################################################\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287126e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1,num_epochs+1), loss_history, label = \"train loss\", linewidth=3)\n",
    "plt.plot(range(1,num_epochs+1), val_loss_history, label = \"val loss\", linewidth=3)\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"loss\", fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135cedc",
   "metadata": {},
   "source": [
    "* __packed padding__: ok\n",
    "    - resolving unreasonable overshooting\n",
    "    - resolving the issue of zero padding and not knowing the original length\n",
    "* __batch normalization__\n",
    "    - so far overshooting\n",
    "* __learning rate__\n",
    "    - a bit large (0.001 -> 0.005) works better\n",
    "* __layer and hidden size__: ok\n",
    "    - not too much large, but larger one works well\n",
    "* last batch dropout\n",
    "    - works okay\n",
    "* __gradient clipping__\n",
    "* __gradient scheduler for decay__\n",
    "* __dropout__: ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30668dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor([[0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018]], device='cuda:0')\n",
      "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "here R correct [ True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      " False  True  True  True  True  True  True  True]\n",
      "labels tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "output tensor([[0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.1018],\n",
      "        [0.0030]], device='cuda:0')\n",
      "pred tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "here R correct [ True  True  True  True  True  True  True  True]\n",
      "labels tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "Test Accuracy of the model on the test data: 92.5 %\n",
      "Precision: 0.8556250000000001\n",
      "Recall: 0.925\n",
      "F1-score: 0.888961038961039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingi/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # correct_r = 0\n",
    "\n",
    "    for batch_sequences, batch_labels in test_loader:\n",
    "        # Data pre-processing\n",
    "        lengths = torch.sum(batch_sequences.sum(dim=2) != 0, dim=1)\n",
    "        batch_sequences = batch_sequences.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch_sequences, lengths)\n",
    "        total += batch_labels.size(0)      \n",
    "\n",
    "        print(\"output {}\".format(output))\n",
    "\n",
    "        ### prediction\n",
    "        if binary_classification:\n",
    "            pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "            correct_tensor = pred.eq(batch_labels.float().view_as(pred))\n",
    "\n",
    "        else: \n",
    "            _, pred = torch.max(output, 1) # multi classifier\n",
    "            correct_tensor = pred.eq(batch_labels.long().view_as(pred))\n",
    "        \n",
    "        # correct_tensor = pred.eq(batch_labels.float().view_as(pred))\n",
    "        current_correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        correct += np.sum(current_correct)\n",
    "\n",
    "        if current_correct.any() == 1:\n",
    "            print(\"pred {}\".format(pred))\n",
    "            print(\"here R correct {}\".format(current_correct))\n",
    "            print(\"labels {}\".format(batch_labels.float()))\n",
    "        \n",
    "        y_true.extend(batch_labels.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "\n",
    "    print('Test Accuracy of the model on the test data: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Calculate other evaluation metrics (precision, recall, F1-score)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-use-lstm-for-a-time-series-classification-task/130559/4\n",
    "# https://notebook.community/Diyago/Machine-Learning-scripts/NLP/LSTM%20RNN/Sentiment%20pytorch/Sentiment_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d24710",
   "metadata": {},
   "source": [
    "### Data input and output setup aimed for ROS topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(final_test_file_idx):\n",
    "    # 1) data loading\n",
    "    unpickled_df = pd.read_pickle(\"./dk_csv_20180101/binary/aisdk_20180101_{}.pkl\".format(str(final_test_file_idx)))\n",
    "    \n",
    "\n",
    "    # 2) data pre-processing\n",
    "    # 2-1) data preparation\n",
    "    unpickled_df['heading_converted']  = np.deg2rad(unpickled_df['Heading'])\n",
    "    # lambda function method: very fast! \n",
    "    # https://stackoverflow.com/questions/71249186/applying-function-to-column-in-a-dataframe\n",
    "\n",
    "    # TODO from ROS no need to conversion of NED to Robotic? \n",
    "    unpickled_df['heading_converted'] = unpickled_df['heading_converted'].apply(convert_from_NED_to_Robotic)\n",
    "    obj_id = unpickled_df['obj_index']\n",
    "\n",
    "    # crop before passing\n",
    "    crop_unpickled_df = get_trajectory_before_pass(unpickled_df, [obj_id[0]])\n",
    "    # print(crop_unpickled_df)\n",
    "\n",
    "    features = crop_unpickled_df[columns_for_x].values.tolist()\n",
    "    label = 0 if crop_unpickled_df[columns_for_y].values[-1][0] == \"L\" else 1 # last row, only char\n",
    "\n",
    "    print(np.shape([features]))\n",
    "    print(\"label {}\".format(label))\n",
    "\n",
    "    # 2-2) data padding\n",
    "    # or should we use [] for batch\n",
    "    pad_feature = pad_sequence([torch.tensor(features)], batch_first=True)\n",
    "    # pad_feature = pad_sequence(torch.tensor(features), batch_first=True)\n",
    "    print(np.shape(pad_feature))\n",
    "\n",
    "    # lengths = torch.sum(pad_feature.sum(dim=2) != 0, dim=1)\n",
    "    # print(lengths)\n",
    "\n",
    "    # prediction\n",
    "    predict(model, pad_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def predict(model, streaming_input, sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - streaming_input: (torch.tensor) [B, L, F]: batch, length, feature_size as padded feature\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO use sequence length later for fixed data streaming length\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "        # Data pre-processing\n",
    "        lengths = torch.sum(streaming_input.sum(dim=2) != 0, dim=1)\n",
    "        streaming_sequences = streaming_input.to(device)\n",
    "        # batch_labels = streaming_input.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch_sequences, lengths)     \n",
    "\n",
    "        ### prediction\n",
    "        # TODO how to output probability\n",
    "        print(\"------------ result ------------\")\n",
    "        if binary_classification:\n",
    "            print(\"before round {}\".format(output))\n",
    "            pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "            \n",
    "            print(\"prdiction value {}\".format(pred))\n",
    "        else: \n",
    "            _, pred = torch.max(output, 1) # multi classifier\n",
    "            correct_tensor = pred.eq(batch_labels.long().view_as(pred))\n",
    "\n",
    "\n",
    "        if (pred.item() == 0):\n",
    "            print(\"obstacle is L passing\")\n",
    "        else:\n",
    "            print(\"obstacls is R passing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 68, 5)\n",
      "label 1\n",
      "torch.Size([1, 68, 5])\n",
      "------------ result ------------\n",
      "before round tensor([[0.1018]], device='cuda:0')\n",
      "prdiction value 0.0\n",
      "obstacle is L passing\n"
     ]
    }
   ],
   "source": [
    "final_test_file_idx = 300\n",
    "final_test(final_test_file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeabcfe4",
   "metadata": {},
   "source": [
    "### Multi class trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ac356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif not binary_classification:\\n    # Training loop\\n    model.train()\\n    loss_history =torch.tensor([])\\n    val_loss_history =torch.tensor([])\\n\\n    for epoch in range(num_epochs):\\n        ##################################################################\\n        ##### training \\n        ##################################################################\\n        for batch_sequences, batch_labels in train_loader:\\n            optimizer.zero_grad()\\n\\n            # 1) Calculate the sequence lengths for the current batch\\n            lengths = torch.sum(batch_sequences.sum(dim=2) != 0, dim=1)\\n\\n            batch_sequences = batch_sequences.to(device)\\n            batch_labels = batch_labels.to(device)\\n            \\n            # 2) Forward pass\\n            output = model(batch_sequences, lengths)\\n            \\n            # this is possible, but I did instead output squeeze to match dimension\\n            # batch_labels = torch.unsqueeze(batch_labels, 1)\\n\\n            # 3) Compute the loss       \\n            # loss = criterion(output.squeeze(), batch_labels.float()) # MJ 2023.07.31\\n            loss = criterion(output.squeeze(), batch_labels.long())\\n\\n            # 4) Backward pass and optimization\\n            loss.backward()\\n            \\n            # 5) Gradient Clipping\\n            # 5-1) Gradient Norm Clipping\\n            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\\n            # 5-2) Gradient Value Clipping\\n            # nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\\n        \\n            optimizer.step()\\n\\n        loss_history = torch.cat([loss_history, torch.tensor([loss.item()]).float()], dim=0)\\n\\n        # Print the loss for every epoch\\n        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}\")\\n\\n        ##################################################################\\n        ##### Validation\\n        ##################################################################\\n        # val_steps = 0\\n        # total_val = 0\\n        # correct_val = 0\\n        # for i, data in enumerate(val_loader, 0):\\n        #     with torch.no_grad():\\n        #         batch_sequences_val, batch_labels_val = data\\n\\n        #         # 1) Calculate the sequence lengths for the current batch\\n        #         lengths_val = torch.sum(batch_sequences_val.sum(dim=2) != 0, dim=1)\\n        #         batch_sequences_val = batch_sequences_val.to(device)\\n        #         batch_labels_val = batch_labels_val.to(device)\\n\\n        #         # 2) Forward pass\\n        #         output_val = model(batch_sequences_val, lengths_val)\\n        #         total_val += batch_labels_val.size(0)      \\n\\n        #         ### prediction\\n                # if binary_classification:\\n                #     pred = torch.round(output.squeeze())  # rounds to the nearest integer\\n                # else: \\n                #     _, pred = torch.max(output, 1) # multi classifier\\n                \\n        #         # correct_tensor_val = pred_val.eq(batch_labels_val.float().view_as(pred_val))\\n        #         correct_tensor_val = pred_val.eq(batch_labels_val.long().view_as(pred_val)) # MJ\\n        #         current_correct_val = np.squeeze(correct_tensor_val.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor_val.cpu().numpy())\\n        #         correct_val += np.sum(current_correct_val)\\n\\n        #         # loss_val = criterion(output_val.squeeze(), batch_labels_val.float())\\n        #         loss_val = criterion(output_val.squeeze(), batch_labels_val.long())\\n        #         val_steps += 1\\n\\n        # val_loss_history = torch.cat([val_loss_history, torch.tensor([loss_val.item()]).float()], dim=0)\\n        # print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {loss_val.item()}\")\\n\\n        \\n        ##################################################################\\n        ##### scheduler\\n        ##################################################################\\n        scheduler.step()\\n\\n    print(\"Finished Training\")\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if not binary_classification:\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    loss_history =torch.tensor([])\n",
    "    val_loss_history =torch.tensor([])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ##################################################################\n",
    "        ##### training \n",
    "        ##################################################################\n",
    "        for batch_sequences, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1) Calculate the sequence lengths for the current batch\n",
    "            lengths = torch.sum(batch_sequences.sum(dim=2) != 0, dim=1)\n",
    "\n",
    "            batch_sequences = batch_sequences.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # 2) Forward pass\n",
    "            output = model(batch_sequences, lengths)\n",
    "            \n",
    "            # this is possible, but I did instead output squeeze to match dimension\n",
    "            # batch_labels = torch.unsqueeze(batch_labels, 1)\n",
    "\n",
    "            # 3) Compute the loss       \n",
    "            # loss = criterion(output.squeeze(), batch_labels.float()) # MJ 2023.07.31\n",
    "            loss = criterion(output.squeeze(), batch_labels.long())\n",
    "\n",
    "            # 4) Backward pass and optimization\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5) Gradient Clipping\n",
    "            # 5-1) Gradient Norm Clipping\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            # 5-2) Gradient Value Clipping\n",
    "            # nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "\n",
    "        loss_history = torch.cat([loss_history, torch.tensor([loss.item()]).float()], dim=0)\n",
    "\n",
    "        # Print the loss for every epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item()}\")\n",
    "\n",
    "        ##################################################################\n",
    "        ##### Validation\n",
    "        ##################################################################\n",
    "        # val_steps = 0\n",
    "        # total_val = 0\n",
    "        # correct_val = 0\n",
    "        # for i, data in enumerate(val_loader, 0):\n",
    "        #     with torch.no_grad():\n",
    "        #         batch_sequences_val, batch_labels_val = data\n",
    "\n",
    "        #         # 1) Calculate the sequence lengths for the current batch\n",
    "        #         lengths_val = torch.sum(batch_sequences_val.sum(dim=2) != 0, dim=1)\n",
    "        #         batch_sequences_val = batch_sequences_val.to(device)\n",
    "        #         batch_labels_val = batch_labels_val.to(device)\n",
    "\n",
    "        #         # 2) Forward pass\n",
    "        #         output_val = model(batch_sequences_val, lengths_val)\n",
    "        #         total_val += batch_labels_val.size(0)      \n",
    "\n",
    "        #         ### prediction\n",
    "                # if binary_classification:\n",
    "                #     pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "                # else: \n",
    "                #     _, pred = torch.max(output, 1) # multi classifier\n",
    "                \n",
    "        #         # correct_tensor_val = pred_val.eq(batch_labels_val.float().view_as(pred_val))\n",
    "        #         correct_tensor_val = pred_val.eq(batch_labels_val.long().view_as(pred_val)) # MJ\n",
    "        #         current_correct_val = np.squeeze(correct_tensor_val.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor_val.cpu().numpy())\n",
    "        #         correct_val += np.sum(current_correct_val)\n",
    "\n",
    "        #         # loss_val = criterion(output_val.squeeze(), batch_labels_val.float())\n",
    "        #         loss_val = criterion(output_val.squeeze(), batch_labels_val.long())\n",
    "        #         val_steps += 1\n",
    "\n",
    "        # val_loss_history = torch.cat([val_loss_history, torch.tensor([loss_val.item()]).float()], dim=0)\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {loss_val.item()}\")\n",
    "\n",
    "        \n",
    "        ##################################################################\n",
    "        ##### scheduler\n",
    "        ##################################################################\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b6f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
